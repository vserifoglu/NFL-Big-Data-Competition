# Big Data Bowl 2026: Analytics Competition

##### Note: I am doing the universit trak ofr this compeitiont. 

## Overview

You are tasked to create a metric or video to analyze NFL player movement during the video frames **after the ball is thrown**.

**Start Date**: 2 months ago  
**Deadline**: 12 days to go

## Description

**Note**: This Big Data Bowl 2026 has two competitions. This competition is the **Analytics competition**. Learn more about the Prediction competition [here](link).

The downfield pass is the crown jewel of American sports. When the ball is in the air, anything can happen, like a touchdown, an interception, or a contested catch. The uncertainty and the importance of the outcome of these plays is what helps keep audiences on the edge of its seat.

The 2026 Big Data Bowl is designed to help the National Football League better understand player movement during the pass play, starting with **when the ball is thrown** and ending when the ball is either caught or ruled incomplete. 

For the offensive team, this means focusing on the targeted receiver, whose job is to move towards the ball landing location in order to complete a catch. For the defensive team, who could have several players moving towards the ball, their jobs are to both prevent the offensive player from making a catch, while also going for the ball themselves. This year's Big Data Bowl asks our fans to help track the movement of these players.

In the **Analytics Competition** of the Big Data Bowl, participants are tasked with describing player movement with the ball in the air in a way that is accessible to coaches and football fans. This could be making a new metric or in the comparison of team or player strategy. As is often the case in Big Data Bowl analytics competitions, participants are encouraged to focus on **one small aspect of the play**, as opposed to trying to capture every aspect of every play. This could include:

- The change of direction of a defensive player
- The acceleration of the receiver
- The way that defensive schemes free players up to make plays

## Timeline

- **September 25, 2025** - Start Date
- **December 17, 2025** - Final Submission Deadline
- **December 18, 2025 - January 19, 2025** - Judging Period*
- **January 20, 2026** - Anticipated Results Announcement

*\*Note: Judging period subject to change based on the number of submissions received*

All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.

## Submission Requirements

A valid submission must contain the following:

1. **Kaggle Writeup**
2. **Media Gallery**
3. **Attached Public Notebook** (University Track)
4. **Attached Public Video** (Broadcast Track)

To create a new Writeup, click on the "New Writeup" button [here](link). After you have saved your Writeup, you should see a "Submit" button in the top right corner.

Your final Submission must be made prior to the deadline. Any un-submitted or draft Writeups by the competition deadline will not be considered by the Judges.

**Note**: If you attach a private Kaggle Resource to your public Kaggle Writeup, your private Resource will automatically be made public after the deadline.

All of the content on Kaggle is written in Markdown. If you are copy-pasting from another application, like Word or your browser, you may need to clean up the markdown or HTML for things to display properly.

### 1. Kaggle Writeup

The Kaggle Writeup serves as your project report. This should include:
- A title
- A subtitle  
- A detailed analysis of your submission

You must select a Track for your Writeup in order to submit.

**Word Limit**: Your Writeup should not exceed 2000 words. Submissions over this limit may be subject to penalty.

The below assets must be attached to the Writeup to be eligible.

#### a. Media Gallery

This is where you should attach any images and/or videos associated with your submission. Writeups should contain **less than ten tables/figures**. Submissions over this limit may be subject to penalty.

You are allowed to submit any tables/figures directly into your Writeup via Markdown editor if preferred, but **a cover image in the media gallery is required** to submit your Writeup.

#### b. Public Notebook

Your code should be submitted as a public notebook in the Project Files field. Your notebook should be publicly accessible and not require a login or paywall. If you use a private Kaggle Notebook, it will automatically be made public after the deadline.

#### c. Video (required for Broadcast Visualization Track)

Attach your video to the Media Gallery. Videos should be **3 minutes or less**, and should be published to YouTube.

## Evaluation Criteria

Participants will select one of two tracks in which to submit:

### **University Track**
University students (college or grad students welcome) are allowed to submit to this track. Participants are challenged to:
- Generally analyze player movement
- Create player or team metrics for either the offensive or defensive units
- Evaluate other passing play topics

### **Broadcast Visualization Track**
Open to any Big Data Bowl participant, the goal of this track is to create an accessible, interesting, and novel visualization that uses the movement of players when the ball is in the air. 

**Note**: Participants in this track can use the data provided, but are also encouraged to overlay film from games (e.g., NFL YouTube).

## Scoring

All entries are evaluated on the following four components, where 0 is the lowest and 10 is the highest.

### Football Score (30%):
- Would NFL teams (or the league office) be able to use these results on a week-to-week basis?
- Does the analysis account for variables that make football data complex?
- Are the ideas unique?

### Data Science Score (30%):
- Is the work correct?
- Are claims backed up by data?
- Are the statistical models appropriate given the data?
- Are the analytical applications innovative?

### Writeup Score (20%):
- Is the Writeup well-written?
- Is the Writeup easy to follow?
- Is the motivation (metric, player evaluation, etc.) clearly defined?

### Data Visualization Score (20%):
- Are the charts and tables provided accessible?
- Are the charts and tables accurate?
- Are the charts and tables innovative?

**Note**: Participants are encouraged to show statistical code if it helps readers better understand their analyses. However, most code should be hidden in an Appendix. Any Writeup that doesn't use the player tracking data will not be scored.
# Competition FAQ: Data Clarifications

## Q1: Clarification on output data
**Question**:  
The output data, is this the data while the ball is explicitly in the air or is it the rest of the play? Is the time the receiver catches the ball the last frame in the output file or does the receiver catch the ball somewhere during the frames in the output file?

**Answer**:  
The output data is explicitly when the ball is in the air.  
The last frame of the output data is when the pass arrives (pass does not always result in a catch).

---

## Q2: Quick Question about Frames
**Question**:  
I hope this wasn't already covered somewhere in the data overview and I simply missed it, but I couldn't find any information about how much time elapses between each frame in the input and output files. Does the data contain a frame every second, a frame every half second, a frame every tenth of a second? Again, apologize if the information is actually somewhere and I just didn't see it.

**Answer**:  
Great question ‚Äî a tenth of a second.

---

## Q3: Pass Forward Frame
**Question**:  
I was wondering which frame is the pass forward moment. Is it the last frame in the input data, or the first frame in the output data?

**Answer**:  
It is the first frame of the output.

---

## Q4: Competition focus and data split before and while the ball is in the air
**Question**:  
In this competition we have a lot of data from before the pass is thrown and a lot less data while the ball is airborne, specifically we have only (x,y) tuple for 5+ frames for 1 receiver and 0‚Äì8 defenders. Of course the topic is "understand player movement while the ball is in the air."

Now here is my question: Is the focus on what player movement before the throw tells us about movement while the ball is in the air, mainly use data while the ball is airborne to understand that specific aspect of the play, or are we more or less free to do whatever analysis we want as long as it's using this year's (tracking) data?

**Answer**:  
Ultimately, the focus is on the relationship between:
- Contextual information in the supplemental data
- Player movement before the ball is thrown
- Player movement after the ball is thrown

However, that does not mean your project must be a prediction of player x‚Äìy coordinates while the ball is in the air (goal of the prediction contest). You are free to focus on specific positions, specific situations in a game, or define "player movement" as something other than x‚Äìy coordinates (or anything else that relates to player movement with ball in air).

---

## Q5: Clarification on output data (similar to Q1)
**Question**:  
The output data, is this the data while the ball is explicitly in the air or is it the rest of the play? Is the time the receiver catches the ball the last frame in the output file or does the receiver catch the ball somewhere during the frames in the output file?

**Answer**:  
The output data is explicitly when the ball is in the air.  
The last frame of the output data is when the pass arrives (pass does not always result in a catch).


Design limitations: 

By design, the output data excludes defenders more than ~8 yards from the landing point, which encodes an assumption that such players have negligible direct impact on the catch or immediate YAC; you can explicitly adopt this as an assumption in your method description.

Methodologically: ‚ÄúWe extend classic pitch‚Äëcontrol / space‚Äëdominance models by decomposing voids into a pre‚Äëthrow global component (scheme/alignment) and a post‚Äëthrow local component (ball‚Äëin‚Äëair reaction), using the competition‚Äôs partial‚Äëtracking setup as a natural separation between the two.‚Äù

Football‚Äëwise: ‚ÄúThis lets us say whether a big completion came from the call (large pre‚Äëthrow void), from execution after the throw (void expanded locally), or from defenders who bailed out bad structure (void collapsed locally), which current public Next Gen‚Äìstyle stats and prior Big Data Bowl work do not isolate this cleanl. 

The "why" behind excluding speed from phase A. 

By baking speed into Phase A, you "Double Count" it. You effectively say, "He is fast, so the play wasn't hard."

You want to say the opposite: "The play WAS hard (wide open), and he solved it BECAUSE he is fast."


Title: 
Given the void at throw (context), here is how this defender‚Äôs chosen path changed the outcome, relative to what similar defenders typically achieve in the same situation 

    "Given the void at throw (context)..."

        Code: S_throw (Distance at Frame 1).

        Concept: The "Mess" they inherited.

    "...here is how this defender‚Äôs chosen path changed the outcome..."

        Code: VIS (Void Improvement Score).

        Concept: The "Cleanup" (Did they close the gap?).

    "...relative to what similar defenders typically achieve..."

        Code: CEOE (Closing Efficiency Over Expectation).

        Concept: The "Benchmark" (Are they elite or just average?).
THE IRRELEVANCE HYPOTHESIS TEST
===============================
Hypothesis: Players are dropped in the Output file because they are far from the landing spot.

1. Distance to Landing Spot (Stats):
          count       mean       std       min        25%        50%        75%        max
status                                                                                    
Ghost     906.0  19.845573  9.067352  0.827644  13.451792  18.329534  25.233789  57.268650
Survivor  331.0   8.422917  5.232197  0.221358   5.023642   7.212006  10.128747  30.418641

2. Danger Ghosts (< 15.0 yards from landing):
   Count: 306
   ‚ö†Ô∏è WARNING: Some players close to the catch point are disappearing!
   Sample of these missing players:
         game_id  play_id         player_role  dist_to_landing
489   2023090700      194  Other Route Runner         7.814054
553   2023090700      194  Other Route Runner         9.613846
617   2023090700      194              Passer        14.950209
734   2023090700      219  Defensive Coverage        12.335518
768   2023090700      219  Defensive Coverage        10.470577
802   2023090700      219  Other Route Runner         4.064836
1227  2023090700      361  Other Route Runner        14.704374
1471  2023090700      436  Defensive Coverage        11.187002
1591  2023090700      436  Other Route Runner         6.416269
1631  2023090700      436              Passer        12.158770
1835  2023090700      461  Other Route Runner        11.254448
1970  2023090700      530  Defensive Coverage        14.062007
2170  2023090700      530  Other Route Runner         7.046432
2268  2023090700      621  Defensive Coverage        11.785045
2297  2023090700      621  Defensive Coverage         8.619294

3. Missing Targeted Receivers: 0
   ‚úÖ PASS: Targeted Receiver always survives.

üèà BIG DATA BOWL 2026: RAW DATASET INSPECTION REPORT
Generated on: 2025-12-05 14:41:28.349025
====================================================

PART 1: SUPPLEMENTARY DATA (METADATA)
-------------------------------------
File: ../data/supplementary_data.csv
Shape: (18009, 41)
Columns: ['game_id', 'season', 'week', 'game_date', 'game_time_eastern', 'home_team_abbr', 'visitor_team_abbr', 'play_id', 'play_description', 'quarter', 'game_clock', 'down', 'yards_to_go', 'possession_team', 'defensive_team', 'yardline_side', 'yardline_number', 'pre_snap_home_score', 'pre_snap_visitor_score', 'play_nullified_by_penalty', 'pass_result', 'pass_length', 'offense_formation', 'receiver_alignment', 'route_of_targeted_receiver', 'play_action', 'dropback_type', 'dropback_distance', 'pass_location_type', 'defenders_in_the_box', 'team_coverage_man_zone', 'team_coverage_type', 'penalty_yards', 'pre_penalty_yards_gained', 'yards_gained', 'expected_points', 'expected_points_added', 'pre_snap_home_team_win_probability', 'pre_snap_visitor_team_win_probability', 'home_team_win_probability_added', 'visitor_team_win_probility_added']

>>> Coverage Types Distribution:
team_coverage_man_zone
ZONE_COVERAGE    12783
MAN_COVERAGE      5221
NaN                  5

>>> Pass Results:
pass_result
C     12470
I      5106
IN      433

>>> Null Values in Critical Columns:
game_id               0
play_id               0
team_coverage_type    5
pass_length           0

PART 2: TRACKING DATA TOPOLOGY
------------------------------
Total Input Files (Pre-Throw): 18
Total Output Files (Post-Throw): 18

PART 3: DEEP SAMPLE ANALYSIS (WEEK 1)
-------------------------------------
Loading Sample Pair:
  - input_2023_w01.csv
  - output_2023_w01.csv

3.1 Column Consistency Check:
‚ö†Ô∏è Schema Mismatch detected!
Only in Input: {'player_name', 's', 'player_to_predict', 'o', 'player_role', 'ball_land_y', 'player_height', 'play_direction', 'player_weight', 'player_side', 'absolute_yardline_number', 'dir', 'ball_land_x', 'a', 'player_position', 'player_birth_date', 'num_frames_output'}
Only in Output: set()

3.2 Sample Statistics (Week 1):
Input Rows (Pre-Throw): 285,714
Output Rows (Post-Throw): 32,088

3.3 Stitching Logic Inspection:
Sample Play (2023090700 - 101):
  - Max Input Frame: 26
  - Min Output Frame: 1 (Should be 1)
  - Gap Analysis: Input ends at 26, Output starts at 1. Offset required.

3.4 Player Roles in Tracking Data:
Defensive Coverage, Other Route Runner, Passer, Targeted Receiver

3.5 Coordinate Bounds (Raw Data):
  - X Range: 1.21 to 119.86
  - Y Range: 0.97 to 52.43
  *Note: If X goes > 100, standard NFL coords (0-120) are likely used.*

3.6 Ball Landing Spot Availability:
  - Percentage of rows with Missing 'ball_land_x': 0.00%
PART 4: AGGREGATE SCAN (ALL WEEKS)
----------------------------------
  - w01: 317,802 rows | 16 games | 819 plays
  - w02: 320,766 rows | 16 games | 850 plays
  - w03: 333,837 rows | 16 games | 904 plays
  - w04: 302,622 rows | 16 games | 779 plays
  - w05: 284,098 rows | 14 games | 742 plays
  - w06: 301,838 rows | 15 games | 793 plays
  - w07: 261,040 rows | 13 games | 693 plays
  - w08: 314,028 rows | 16 games | 827 plays
  - w09: 281,087 rows | 14 games | 711 plays
  - w10: 289,380 rows | 14 games | 721 plays
  - w11: 271,036 rows | 14 games | 707 plays
  - w12: 327,096 rows | 16 games | 854 plays
  - w13: 263,323 rows | 13 games | 666 plays
  - w14: 312,845 rows | 15 games | 796 plays
  - w15: 314,535 rows | 16 games | 791 plays
  - w16: 352,925 rows | 16 games | 896 plays
  - w17: 310,658 rows | 16 games | 809 plays
  - w18: 284,599 rows | 16 games | 750 plays

TOTAL DATASET STATS:
  - Total Rows: 5,443,515
  - Unique Games: 272
  - Unique Plays: 14108


üèà BIG DATA BOWL 2026: RAW DATASET INSPECTION REPORT
====================================================

Files Found: 18 Input pairs, 18 Output pairs.

PART 3: WEEK 1 DEEP DIVE & SPARSITY PROOF
-----------------------------------------
3.7 PLAYER DROPOUT & TEMPORAL SPARSITY ANALYSIS
   (Proving that pre_throw frames > post_throw and identifying missing players)

   A. Temporal Differences (Frame Counts):
      - Avg Pre-Throw Frames:  28.27
      - Avg Post-Throw Frames: 11.06
      - Conclusion: On average, input files are 2.89x longer than output files.

   B. Entity Dropout (Players vanishing in Output):
      - Total Plays Analyzed: 819
      - Plays with AT LEAST ONE missing player: 819 (100.00%)
      - Avg Missing Players per Play: 9.05
      - Max Missing Players in a single play: 13

   C. Profile of Vanishing Players (Sample):
      Sample Play (Game 2023091004, Play 219):
      Missing 13 players in Output.
 nfl_id      player_name        player_role player_position
  54583    Akayleb Evans Defensive Coverage              CB
  47816     Byron Murphy Defensive Coverage              CB
  53554     Camryn Bynum Defensive Coverage              FS
  38559   Harrison Smith Defensive Coverage              FS
  42427     Jordan Hicks Defensive Coverage             ILB
  52613    Josh Metellus Defensive Coverage              SS
  55967   Mekhi Blackmon Defensive Coverage              CB
  53519 Patrick Jones II Defensive Coverage             OLB
  54571       Cade Otton Other Route Runner              TE
  44896 Chris Godwin Jr. Other Route Runner              WR
  55075  Deven Thompkins Other Route Runner              WR
  41233       Mike Evans Other Route Runner              WR
  46070   Baker Mayfield             Passer              QB

      Observation: 0.0% of missing players in this sample are Linemen.
      (Hypothesis: Tracking data often drops interior linemen post-throw if they are not near the play.)

Inspection complete. See 'data_inspection_results.txt' for details.
# üèà Big Data Bowl 2026: Dataset Inspection & Engineering Handbook

**Project:** The Void Engine (Zone Coverage Auditing)
**Domain:** NFL Player Tracking Data (Post-Snap/Post-Throw)
**Status:** VALIDATED (with documented constraints)

---

## 1. File Topology & Data Flow

The dataset is split into **Tracking Data** (Time-series) and **Context Data** (Play-level metadata).

### Directory Structure
* **`data/train/`**: Contains the raw tracking data.
    * `input_w[01-09].csv`: **Pre-throw** tracking data (frames leading up to the pass).
    * `output_w[01-09].csv`: **Post-throw** tracking data (frames where the ball is in the air).
    * *Note: Files are paired by Week. Week 1 Input matches Week 1 Output.*
* **`data/supplementary_data.csv`**: The "Rosetta Stone." Contains play-level labels (Coverages, EPA, Penalties).

### The "Stitch" & Time Sparsity
* **Temporal Ratio:** `Input` files are ~2.9x longer than `Output` files.
* **Frame Zeroing:** `input_*.csv` frames end at $N$. The first frame of `output_*.csv` is $1$. The pipeline offsets the output frames by $N$ to create a continuous timeline $1 \to N \to End$.

---

## 2. The "Ghost" Constraint (Attrition Analysis)

**Critical Finding:** The dataset is **Lossy**. Players present in the `Input` file (pre-throw) frequently disappear in the `Output` file (post-throw).

### The Survival Rules (Empirically Proven)
1.  **Targeted Receivers:** 100% Survival Rate. They never disappear.
2.  **Proximity Filter:** The dataset aggressively drops players far from the ball.
    * *Survivors* (Kept in data): Avg distance to landing spot = **8.4 yards**.
    * *Ghosts* (Dropped from data): Avg distance to landing spot = **19.8 yards**.
3.  **The "Danger Zone" (8-15 yards):**
    * There is a transition zone (8-15 yards from catch) where players *may* disappear.
    * *Implication:* If a defender is ~12 yards away from the catch, they might not be graded by the Void Engine because they do not exist in the dataframe.

**Engineering Decision:** The Void Engine treats "Ghosts" as **Exempt**. We assume if the NFL tracking system deemed them irrelevant enough to drop, they were not the primary cause of the defensive breakdown.

---

## 3. Coordinate System & Physics

**Warning:** Raw NFL data is non-standardized regarding direction. This pipeline enforces a **Left-to-Right** normalization.

### Normalization Logic
If `play_direction == 'left'`, the engine flips the geometry so the offense always drives from Left ($X=0$) to Right ($X=120$).

* **X-Axis:** $X_{new} = 120 - X_{raw}$
* **Y-Axis:** $Y_{new} = 53.3 - Y_{raw}$
* **Orientation/Dir:** $\theta_{new} = (\theta_{raw} + 180) \mod 360$

### Derived Geometry
* **Line of Scrimmage (LOS):** Calculated as `ball_land_x - pass_length`.
* **Depth:** $X_{player} - X_{LOS}$.
* **Compression Factor:** Used for "Red Zone Squash."
    $$\text{Compression} = \text{clip}\left(\frac{110 - \text{LOS}}{20}, 0.5, 1.0\right)$$
# Dataset Description

## Summary of Data

Here, you'll find a summary of each data set in the 2026 Big Data Bowl, a list of key variables to join on, and a description of each variable. The tracking data is provided by the NFL Next Gen Stats team.

## External Data

The 2026 Big Data Bowl allows participants to use external NFL data as long as it is free and publicly available to all participants. Examples of sources that could be used include:

- **nflverse**
- **Pro Football Reference**

**Please note**: The `gameId` and `playId` of the Big Data Bowl data merges with the `old_game_id` and `play_id` of nflverse's play-by-play data.

## Files

### `train/`

#### `input_2023_w[01-18].csv`
The input data contains tracking data **before the pass is thrown**.

| Variable | Description | Type |
|----------|-------------|------|
| `game_id` | Game identifier, unique | Numeric |
| `play_id` | Play identifier, not unique across games | Numeric |
| `player_to_predict` | Whether or not the x/y prediction for this player will be included in the corresponding output file | Boolean |
| `nfl_id` | Player identification number, unique across players | Numeric |
| `frame_id` | Frame identifier for each play/type, starting at 1 for each `game_id`/`play_id`/file type (input or output) | Numeric |
| `play_direction` | Direction that the offense is moving (left or right) | Text |
| `absolute_yardline_number` | Distance from end zone for possession team | Numeric |
| `player_height` | Player height (ft-in) | Text |
| `player_name` | Player name | Text |
| `player_weight` | Player weight (lbs) | Numeric |
| `player_birth_date` | Birth date (yyyy-mm-dd) | Date |
| `player_position` | The player's position (the specific role on the field that they typically play) | Text |
| `player_side` | Team player is on (Offense or Defense) | Text |
| `player_role` | Role player has on play (Defensive Coverage, Targeted Receiver, Passer or Other Route Runner) | Text |
| `x` | Player position along the long axis of the field, generally within 0 - 120 yards | Numeric |
| `y` | Player position along the short axis of the field, generally within 0 - 53.3 yards | Numeric |
| `s` | Speed in yards/second | Numeric |
| `a` | Acceleration in yards/second¬≤ | Numeric |
| `o` | Orientation of player (degrees) | Numeric |
| `dir` | Angle of player motion (degrees) | Numeric |
| `num_frames_output` | Number of frames to predict in output data for the given `game_id`/`play_id`/`nfl_id` | Numeric |
| `ball_land_x` | Ball landing position along the long axis of the field, generally within 0 - 120 yards | Numeric |
| `ball_land_y` | Ball landing position along the short axis of the field, generally within 0 - 53.3 yards | Numeric |

#### `output_2023_w[01-18].csv`
The output data contains tracking data **after the pass is thrown**.

| Variable | Description | Type |
|----------|-------------|------|
| `game_id` | Game identifier, unique | Numeric |
| `play_id` | Play identifier, not unique across games | Numeric |
| `nfl_id` | Player identification number, unique across players | Numeric |
| `frame_id` | Frame identifier for each play/type, starting at 1 for each `game_id`/`play_id`/file type (input or output). The maximum value for a given `game_id`, `play_id` and `nfl_id` will be the same as the `num_frames_output` value from the corresponding input file | Numeric |
| `x` | Player position along the long axis of the field, generally within 0 - 120 yards | Numeric |
| `y` | Player position along the short axis of the field, generally within 0 - 53.3 yards | Numeric |

### Supplementary
Contextual info about the game/play.

| Variable | Description | Type |
|----------|-------------|------|
| `game_id` | Game identifier, unique | Numeric |
| `season` | Season of game | Numeric |
| `week` | Week of game | Numeric |
| `game_date` | Game Date (mm/dd/yyyy) | Date |
| `game_time_eastern` | Start time of game (HH:MM:SS, EST) | Time |
| `home_team_abbr` | Home team three-letter code | Text |
| `visitor_team_abbr` | Visiting team three-letter code | Text |
| `home_final_score` | The total amount of points scored by the home team in the game | Numeric |
| `visitor_final_score` | The total amount of points scored by the visiting team in the game | Numeric |
| `play_id` | Play identifier, not unique across games | Numeric |
| `play_description` | Description of play | Text |
| `quarter` | Game quarter | Numeric |
| `game_clock` | Time on clock of play (MM:SS) | Time |
| `down` | Down | Numeric |
| `yards_to_go` | Distance needed for a first down | Numeric |
| `possession_team` | Team abbr of team on offense with possession of ball | Text |
| `defensive_team` | Team abbr of team on defense | Text |
| `yardline_side` | 3-letter team code corresponding to line-of-scrimmage | Text |
| `yardline_number` | Yard line at line-of-scrimmage | Numeric |
| `pre_snap_home_score` | Home score prior to the play | Numeric |
| `pre_snap_visitor_score` | Visiting team score prior to the play | Numeric |
| `pass_result` | Dropback outcome of the play (C: Complete pass, I: Incomplete pass, S: Quarterback sack, IN: Intercepted pass, R: Scramble) | Text |
| `play_nullified_by_penalty` | Whether or not an accepted penalty on the play cancels the play outcome (Y stands for yes and N stands for no) | Text |
| `pass_length` | The distance beyond the LOS that the ball traveled not including yards into the endzone. If thrown behind LOS, the value is negative | Numeric |
| `offense_formation` | Formation used by possession team | Text |
| `receiver_alignment` | Enumerated as 0x0, 1x0, 1x1, 2x0, 2x1, 2x2, 3x0, 3x1, 3x2 | Text |
| `route_of_targeted_receiver` | Route ran by targeted receiver | Text |
| `play_action` | Indicates if there was play action on the play | Binary |
| `dropback_type` | The type of drop back after the snap by the QB (Traditional, Designed Rollout, Scramble, Scramble Rollout, Designed Rollout Left, Designed Rollout Right, Scramble Rollout Left, Scramble Rollout Right, Designed Run, QB Draw, Rollout) | Text |
| `dropback_distance` | The distance the QB dropped back (yards) behind the center after the snap | Numeric |
| `pass_location_type` | The location type of where the QB was at the time of throw - InsideTackle Box, Outside Left, Outside Right or Unknown | Text |
| `defenders_in_the_box` | Number of defenders in close proximity to line-of-scrimmage | Numeric |
| `team_coverage_man_zone` | Indicates the overarching type of coverage (Man/Zone) on a play | Text |
| `team_coverage_type` | The specific kind of coverage assigned on the play | Text |
| `penalty_yards` | Yards gained by offense by penalty | Numeric |
| `pre_penalty_yards_gained` | Net yards gained by the offense, before penalty yardage | Numeric |
| `yards_gained` | Net yards gained by the offense, including penalty yardage | Numeric |
| `expected_points` | Expected points on this play | Numeric |
| `expected_points_added` | Delta of expected points on this play | Numeric |
| `pre_snap_home_team_win_probability` | The win probability of the home team before the play | Numeric |
| `pre_snap_visitor_team_win_probability` | The win probability of the visiting team before the play | Numeric |
| `home_team_win_probability_added` | Win probability delta for home team | Numeric |
| `visitor_team_win_probility_added` | Win probability delta for visitor team | Numeric |
PHYSICS VARIABLES AVAILABILITY REPORT
=======================================

1. Column Existence Check:
   Input Files Missing: None (All present)
   Output Files Missing: ['s', 'a', 'dir', 'o']

2. Data Content Check (Are they full of NaNs?):
   [INPUT FILES]
     - s: 0.0% Nulls
     - a: 0.0% Nulls
     - dir: 0.0% Nulls
     - o: 0.0% Nulls

   [OUTPUT FILES]
     - s: COLUMN MISSING
     - a: COLUMN MISSING
     - dir: COLUMN MISSING
     - o: COLUMN MISSING

3. CRITICAL IMPLICATIONS:
   The Output files lack motion vectors.
   IMPACT 1 (Animation): You cannot use 'dir' arrows in visualization.
   IMPACT 2 (Normalization): 'data_preprocessor.py' tries to flip 'dir'/'o' for left-moving plays.
             If these columns are missing, that code block will crash.
   IMPACT 3 (Physics): You must DERIVE speed/direction from x,y changes if needed.


import pandera.pandas as pa
from pandera.typing import Series

# TODO: double check comments after modifications.

class RawSuppSchema(pa.DataFrameModel):
    """
    Validates 'supplementary_data.csv'.
    Contains Game Context, Play Types, and Outcomes.
    """
    # --- Identifiers ---
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    
    # --- Context ---
    week: Series[int] = pa.Field(coerce=True)
    home_team_abbr: Series[str]
    visitor_team_abbr: Series[str]
    
    # --- Game State ---
    down: Series[int] = pa.Field(coerce=True, ge=1, le=4)
    yards_to_go: Series[int] = pa.Field(coerce=True)
    possession_team: Series[str]
    yardline_side: Series[str] = pa.Field(nullable=True)
    yardline_number: Series[int] = pa.Field(ge=0, le=50) 
    defensive_team: Series[str] = pa.Field(nullable=True)
    
    # --- Win Probability (Used for filters) ---
    pre_snap_home_team_win_probability: Series[float] = pa.Field(nullable=True)
    pre_snap_visitor_team_win_probability: Series[float] = pa.Field(nullable=True)

    # --- Logic Filters ---
    play_nullified_by_penalty: Series[str] = pa.Field(nullable=True)
    dropback_type: Series[str] = pa.Field(nullable=True) 
    team_coverage_man_zone: Series[str] = pa.Field(nullable=True)
    team_coverage_type: Series[str] = pa.Field(nullable=True) 
    pass_result: Series[str] = pa.Field(nullable=True) 
    pass_length: Series[int] = pa.Field(nullable=True)
    route_of_targeted_receiver: Series[str] = pa.Field(nullable=True)
    yards_gained: Series[int] = pa.Field(coerce=True, nullable=True)
    expected_points_added: Series[float] = pa.Field(coerce=True, nullable=True)

    class Config:
        strict = 'filter' 


class RawTrackingSchema(pa.DataFrameModel):
    """
    Validates 'input_wXX.csv' files (Pre-Throw).
    """
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    frame_id: Series[int] = pa.Field(coerce=True, ge=1)
    nfl_id: Series[float] = pa.Field(coerce=True, nullable=True) # Nullable for Ball

    # --- Normalization Anchors ---
    play_direction: Series[str] 
    player_name: Series[str]
    absolute_yardline_number: Series[int] = pa.Field(ge=0, le=120, nullable=True)
    
    # --- Player Attributes ---
    player_role: Series[str] = pa.Field(nullable=True)
    player_position: Series[str] = pa.Field(nullable=True)
    
    # --- Physics Vectors (Raw) ---
    x: Series[float] = pa.Field(ge=0, nullable=True)
    y: Series[float] = pa.Field(ge=0, nullable=True)
    s: Series[float] = pa.Field(ge=0, nullable=True) 
    
    # --- Targets ---
    ball_land_x: Series[float] = pa.Field(nullable=True)
    ball_land_y: Series[float] = pa.Field(nullable=True)

    class Config:
        strict = 'filter' 


class OutputTrackingSchema(pa.DataFrameModel):
    """
    Validates 'output_wXX.csv' files (Post-Throw).
    Minimal schema since physics are missing.
    """
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    nfl_id: Series[float] = pa.Field(coerce=True, nullable=True)
    frame_id: Series[int] = pa.Field(coerce=True)
    
    x: Series[float] = pa.Field(ge=0, nullable=True)
    y: Series[float] = pa.Field(ge=0, nullable=True)

    class Config:
        strict = 'filter'


class PreprocessedSchema(RawTrackingSchema, RawSuppSchema):
    """
    Validates the output of 'preprocessing.py'.
    Combined Schema + Derived Features.
    """
    phase: Series[str] = pa.Field(isin=["pre_throw", "post_throw"])
    
    # [NEW] Field Logic Feature (0-100 scale)
    yards_from_own_goal: Series[int] = pa.Field(ge=0, le=100, nullable=True)
    
    # [NEW] Win Probability of Possession Team (0-1)
    possession_win_prob: Series[float] = pa.Field(ge=0, le=1, nullable=True)

    class Config:
        strict = 'filter'


class PhysicsSchema(PreprocessedSchema):
    """
    Validates the output of 'physics_engine.py'.
    Adds derived kinematics from Savitzky-Golay.
    """
    # Derived from X,Y deltas
    s_derived: Series[float] = pa.Field(nullable=True, coerce=True) # Speed
    a_derived: Series[float] = pa.Field(nullable=True, coerce=True) # Accel
    
    # Note: 'dir' and 'o' are REMOVED because we rely on closing vectors now.
    
    class Config:
        strict = 'filter'


class AggregationScoresSchema(pa.DataFrameModel):
    """
    Validates the 'Score Card' subset merged onto the animation frames.
    Ensures every frame knows the context (Void) and the result (Eraser Score).
    """
    # Identifiers
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    nfl_id: Series[float] = pa.Field(coerce=True)

    # Phase A Metrics (Context)
    dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True)
    void_type: Series[str] = pa.Field(isin=["High Void", "Tight Window", "Neutral"], nullable=True)

    # Phase B Metrics (Results)
    vis_score: Series[float] = pa.Field(nullable=True)  
    ceoe_score: Series[float] = pa.Field(nullable=True)

    class Config:
        strict = 'filter'


class FullPlayAnimationSchema(PhysicsSchema, AggregationScoresSchema): 

    class Config:
        strict = 'filter'


class ContextSchema(pa.DataFrameModel):
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    
    target_nfl_id: Series[float] = pa.Field(nullable=True)
    nearest_def_nfl_id: Series[float] = pa.Field(nullable=True)
    dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True) 
    void_type: Series[str] = pa.Field(isin=["High Void", "Tight Window", "Neutral"], nullable=True)

    class Config:
        strict = 'filter'


class EraserMetricsSchema(pa.DataFrameModel):
    """
    PHASE B: The Action.
    One row per Play/Defender. Defines the closing efficiency.
    """
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    nfl_id: Series[float] = pa.Field(coerce=True)
    
    p_dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True)
    
    # S_arrival
    dist_at_arrival: Series[float] = pa.Field() 
    
    # Total yards gained on target
    distance_closed: Series[float] = pa.Field() 

    # Yards per second toward target
    avg_closing_speed: Series[float] = pa.Field() 
    
    # Void Improvement Score (S_throw - S_arrival)
    vis_score: Series[float] = pa.Field() 

    class Config:
        strict = 'filter'


class BenchMarkingSchema(pa.DataFrameModel):
    """
    Validates the Player Metadata (Static info) to be attached to the final report.
    Used to select columns dynamically in the orchestrator.
    """
    # Keys
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    nfl_id: Series[float] = pa.Field(coerce=True)
    
    # Metadata
    player_role: Series[str] = pa.Field()
    player_name: Series[str] = pa.Field(nullable=True)
    player_position: Series[str] = pa.Field()
    week: Series[int] = pa.Field(coerce=True) 
    down: Series[int] = pa.Field(coerce=True)
    team_coverage_type: Series[str]
    pass_result: Series[str] = pa.Field(nullable=True)
    yards_gained: Series[int] = pa.Field(coerce=True, nullable=True)
    pass_length: Series[int] = pa.Field(coerce=True, nullable=True)
    expected_points_added: Series[float] = pa.Field(coerce=True, nullable=True)

    class Config:
        strict = 'filter'


class AnalysisReportSchema(pa.DataFrameModel):
    """
    Validates the Player Metadata (Static info) to be attached to the final report.
    Used to select columns dynamically in the orchestrator.
    """
    # keys
    game_id: Series[int]
    play_id: Series[int]
    nfl_id: Series[float]
    
    # Meta
    player_position: Series[str] = pa.Field(nullable=False) # Must exist for benchmarking
    player_name: Series[str] = pa.Field(nullable=True)
    player_role: Series[str]
    team_coverage_type: Series[str]
    down: Series[int]
    pass_result: Series[str] = pa.Field(nullable=True)
    dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True)
    dist_at_arrival: Series[float] = pa.Field(ge=0, nullable=True)
    yards_gained: Series[int] = pa.Field(coerce=True, nullable=True)
    pass_length: Series[int] = pa.Field(coerce=True, nullable=True)
    expected_points_added: Series[float] = pa.Field(coerce=True, nullable=True)

    # Context
    void_type: Series[str] = pa.Field(isin=["High Void", "Tight Window", "Neutral"], nullable=True)
    
    # Metrics
    vis_score: Series[float]
    avg_closing_speed: Series[float]
    p_dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True)

    ceoe_score: Series[float] = pa.Field(nullable=False) # Should never be NaN

    class Config:
        strict = 'filter' 
import os
import glob
import re
import pandas as pd
from typing import Generator, Tuple
from schema import RawTrackingSchema, OutputTrackingSchema, RawSuppSchema


class DataLoader:
    def __init__(self, data_dir: str, supp_file: str):
        """
        Scans the directory for files but DOES NOT load them yet.
        """
        self.data_dir = data_dir
        self.supp_file = supp_file
        
        # 1. Find all files
        self.input_files = sorted(glob.glob(os.path.join(self.data_dir, 'input_*.csv')))
        self.output_files = glob.glob(os.path.join(self.data_dir, 'output_*.csv'))
        
        self.output_map = {}
        for f in self.output_files:
            match = re.search(r'w(\d{2})', f)
            if not match:
                continue
            self.output_map[match.group(1)] = f

    def load_supplementary(self) -> pd.DataFrame:
        """
        Loads the single Supplementary file.
        """
        if not os.path.exists(self.supp_file):
            raise FileNotFoundError(f"Missing Supp File: {self.supp_file}")
            
        df = pd.read_csv(self.supp_file, low_memory=False)
            
        return RawSuppSchema.validate(df)

    def stream_weeks(self) -> Generator[Tuple[str, pd.DataFrame, pd.DataFrame], None, None]:
        """
        The Lazy Loader.
        Yields: (week_num, input_df, output_df)
        
        Validation happens JUST-IN-TIME here.
        """

        count = 0
        for input_path in self.input_files:
            # if count > 0: break
            # Extract Week Number
            match = re.search(r'w(\d{2})', input_path)
            
            if not match: continue
            week_num = match.group(1)
            
            output_path = self.output_map.get(week_num)

            print(f"Streaming Week {week_num}...")
            
            # Load from Disk
            input_raw = pd.read_csv(input_path, low_memory=False)
            output_raw = pd.read_csv(output_path, low_memory=False)
            
            input_raw['nfl_id'] = pd.to_numeric(input_raw['nfl_id'], errors='coerce')
            output_raw['nfl_id'] = pd.to_numeric(output_raw['nfl_id'], errors='coerce')

            # VALIDATE
            input_valid = RawTrackingSchema.validate(input_raw)
            output_valid = OutputTrackingSchema.validate(output_raw)
            
            # count += 1
            # Yield the clean, validated data to the Orchestrator
            yield week_num, input_valid, output_valid
import pandas as pd
import numpy as np
import gc
from typing import Generator, Tuple, List
from schema import PreprocessedSchema

class DataPreProcessor:
    def __init__(self):
        self.output_schema = PreprocessedSchema
        self.keep_cols = list(self.output_schema.to_schema().columns.keys())

    def filter_context(self, supp_df):
        """
        Filters the supplementary dataframe and performs 'Lightweight Feature Engineering'.
        Goal: Create the 'Base Subset' (Standard Football) without loading tracking data.
        """

        # 1. Calculate Possession Win Probability
        supp_df['possession_win_prob'] = np.where(
            supp_df['possession_team'] == supp_df['home_team_abbr'],
            supp_df['pre_snap_home_team_win_probability'],
            supp_df['pre_snap_visitor_team_win_probability'],
        )
        
        # 2. Calculate Normalized Field Position (0-100 Scale)
        # Logic: If on own side, use number. If on opp side, use 100 - number.
        supp_df['yards_from_own_goal'] = np.where(
            supp_df['yardline_side'] == supp_df['possession_team'],
            supp_df['yardline_number'],           
            100 - supp_df['yardline_number']      
        )


        # valid mask filters
        valid_mask = (
            (supp_df['team_coverage_man_zone'].astype(str).str.contains('Zone', case=False, na=False)) &
            (supp_df['pass_result'].isin(['C', 'I', 'IN'])) &
            (supp_df['team_coverage_type'] != 'COVER_6_ZONE') &
            (~supp_df['dropback_type'].str.upper().isin([
                'SCRAMBLE', 'SCRAMBLE_ROLLOUT_LEFT', 'SCRAMBLE_ROLLOUT_RIGHT', 'QB_DRAW'])) &
            (supp_df['play_nullified_by_penalty'] != 'Y')
        )

        # remove trick / cheap plays
        screen_shovel_mask = (
            # Text Search for Screens
            supp_df['route_of_targeted_receiver'].astype(str).str.upper().str.contains('SCREEN', na=False) | 
            
            # Physics Check: Ball caught behind or at LOS (Shovels/Swings)
            (supp_df['pass_length'] <= 0) | 
            
            # Check-downs (Flat routes < 3 yards)
            (
                (supp_df['route_of_targeted_receiver'].astype(str).str.upper() == 'FLAT') & 
                (supp_df['pass_length'] < 3)
            )
        )
        
        # base situations
        base_situation_mask = (           
            # Standard Downs
            (supp_df['down'].isin([1, 2])) &
            
            # Competitive Game (Neutral Script)
            (supp_df['possession_win_prob'].between(0.20, 0.80)) & 

            # On Schedule (Not 1st & 20 or 2nd & 18)
            (supp_df['yards_to_go'] <= 10) &
            
            # We use our new engineered column here
            (supp_df['yards_from_own_goal'].between(20, 80))
        )
        
        final_valid_mask = (
            valid_mask &
            (~screen_shovel_mask) &
            base_situation_mask
        )

        return supp_df[final_valid_mask].copy()

    def _stitch_tracking_data(self, input_df, output_df, valid_keys):
        """
        Pure Logic. Merges Pre-Throw and Post-Throw data.
        """
        # Filter Input
        input_df['key_tuple'] = list(zip(input_df.game_id, input_df.play_id))
        input_df = input_df[input_df['key_tuple'].isin(valid_keys)].drop(columns=['key_tuple'])
        input_df['phase'] = 'pre_throw'
        
        if output_df.empty: return input_df

        # Filter Output
        output_df['key_tuple'] = list(zip(output_df.game_id, output_df.play_id))
        output_df = output_df[output_df['key_tuple'].isin(valid_keys)].drop(columns=['key_tuple'])
        
        # Logic: Tag first frame of Output as pass_forward
        output_df['event'] = None
        output_df.loc[output_df['frame_id'] == 1, 'event'] = 'pass_forward'
        
        # TODO: move this to schema
        # Metadata Propagation (Players missing in Output get this from Input)
        meta_cols = ['game_id', 'play_id', 'nfl_id', 'player_name', 'jersey_number', 'player_position', 
                     'player_role', 'player_side', 'play_direction', 'absolute_yardline_number', 
                     'ball_land_x', 'ball_land_y']
        
        avail_cols = [c for c in meta_cols if c in input_df.columns]
        player_meta = input_df[avail_cols].drop_duplicates(subset=['game_id', 'play_id', 'nfl_id'])
        
        # Frame Offset Calculation
        play_offsets = input_df.groupby(['game_id', 'play_id'])['frame_id'].max().reset_index()
        play_offsets.columns = ['game_id', 'play_id', 'offset']
        
        output_df = output_df.merge(player_meta, on=['game_id', 'play_id', 'nfl_id'], how='left')
        output_df = output_df.merge(play_offsets, on=['game_id', 'play_id'], how='left')
        
        # Apply Offset
        output_df['frame_id'] = output_df['frame_id'] + output_df['offset'].fillna(0)
        output_df['phase'] = 'post_throw'
        
        df = pd.concat([input_df, output_df.drop(columns=['offset'])], ignore_index=True)

        return df

    def _normalize_coordinates(self, df):
        """
        Standardizes field geometry to Left->Right drive direction.
        """
        if 'play_direction' not in df.columns: return df
        mask = df['play_direction'].str.lower() == 'left'
        
        for col in ['x', 'ball_land_x']:
            if col in df.columns: df.loc[mask, col] = 120 - df.loc[mask, col]

        for col in ['y', 'ball_land_y']:
            if col in df.columns: df.loc[mask, col] = 53.3 - df.loc[mask, col]

        return df

    def _clean_and_deduplicate(self, df):
        """
        Ensures strict temporal ordering and removes duplicate frames at the stitch point.
        """
        df['phase_rank'] = df['phase'].apply(lambda x: 1 if x == 'pre_throw' else 2)        
        
        df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id', 'phase_rank'])        
        
        df = df.drop_duplicates(subset=['game_id', 'play_id', 'nfl_id', 'frame_id'], keep='last')
        
        return df.drop(columns=['phase_rank'])

    def process_single_week(self, week_num, input_df, output_df, context_df):
        """
        Internal logic for a single week.
        """
        valid_keys = set(zip(context_df.game_id, context_df.play_id))

        week_df = self._stitch_tracking_data(input_df, output_df, valid_keys)

        week_df = week_df.merge(context_df, on=['game_id', 'play_id'], how='inner')

        week_df = self._normalize_coordinates(week_df)

        week_df['los_x'] = week_df['ball_land_x'] - week_df['pass_length']
        week_df['week'] = int(week_num)

        week_df = self._clean_and_deduplicate(week_df)

        return self.output_schema.validate(week_df)

    def run(self, data_stream: Generator[Tuple[str, pd.DataFrame, pd.DataFrame], None, None], 
            raw_context_df: pd.DataFrame) -> pd.DataFrame:
        """
        MAIN ENTRY POINT.
        """
        clean_context = self.filter_context(raw_context_df)
        
        processed_chunks: List[pd.DataFrame] = []
        for week_num, input_df, output_df in data_stream:
            
            clean_week_df = self.process_single_week(week_num, input_df, output_df, clean_context)

            if not clean_week_df.empty:
                processed_chunks.append(clean_week_df)

            del input_df, output_df
            gc.collect()

        if not processed_chunks:
            return pd.DataFrame()
        
        return pd.concat(processed_chunks, ignore_index=True)
import pandas as pd
import numpy as np
from scipy.signal import savgol_filter
from schema import PhysicsSchema

class PhysicsEngine:
    def __init__(self):
        self.output_schema = PhysicsSchema

    def derive_metrics(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Applies Savitzky-Golay filter to calculate generic Speed (s) and Acceleration (a).
        REMOVED: Direction (dir) calculation, as we now use specific vectors in Phase B.
        """
        # Ensure temporal ordering for the filter
        df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])
        
        # SAVITZKY-GOLAY PARAMETERS
        WINDOW = 7 # 0.7 seconds
        POLY = 2   # Quadratic fit
        
        def calculate_sg(group):
            # Short Track Handling
            if len(group) < WINDOW:
                # Simple Euclidean distance change / 0.1s
                dx = group['x'].diff() 
                dy = group['y'].diff()
                
                dist = np.sqrt(dx**2 + dy**2)
                s = dist / 0.1  # Speed in yds/s
                
                # Acceleration is diff of speed
                a = s.diff().fillna(0) / 0.1
                
                return pd.DataFrame(
                    {'s_derived': s, 'a_derived': a}, 
                    index=group.index)
            
            # 1. First Derivative (Velocity)
            vx = savgol_filter(group['x'], window_length=WINDOW, polyorder=POLY, deriv=1, delta=0.1)
            vy = savgol_filter(group['y'], window_length=WINDOW, polyorder=POLY, deriv=1, delta=0.1)
            
            # 2. Second Derivative (Acceleration)
            ax = savgol_filter(group['x'], window_length=WINDOW, polyorder=POLY, deriv=2, delta=0.1)
            ay = savgol_filter(group['y'], window_length=WINDOW, polyorder=POLY, deriv=2, delta=0.1)
            
            # 3. Magnitudes (Scalar)
            s = np.sqrt(vx**2 + vy**2)
            a = np.sqrt(ax**2 + ay**2)
            
            return pd.DataFrame({
                's_derived': s,
                'a_derived': a
            }, index=group.index)

        # Apply grouping
        # Only apply to players (nfl_id is not null)
        mask_players = df['nfl_id'].notna()
        
        physics_cols = df[mask_players].groupby(
            ['game_id', 'play_id', 'nfl_id'], group_keys=False).apply(calculate_sg, include_groups=False)

        # Map back to original DataFrame
        df.loc[physics_cols.index, 's_derived'] = physics_cols['s_derived']
        df.loc[physics_cols.index, 'a_derived'] = physics_cols['a_derived']

        return self.output_schema.validate(df)
import pandas as pd
import numpy as np
from schema import ContextSchema

class ContextEngine:
    def __init__(self):
        self.output_schema = ContextSchema

    def calculate_void_context(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        PHASE A: Calculates S_throw.
        capture ALL players at the moment of the throw.
        """
        # 1. Filter for Pre-Throw phase
        pre_throw_mask = df['phase'] == 'pre_throw'
        df_pre = df[pre_throw_mask].copy()

        # IDENTIFY THE SNAPSHOT FRAME
        # we calculate the MAX frame_id for every play
        last_frame_ids = df_pre.groupby(['game_id', 'play_id'])['frame_id'].transform('max')
        
        # We keep rows where the frame_id matches the last frame of that play
        throw_frames = df_pre[df_pre['frame_id'] == last_frame_ids].copy()

        # TODO: delete
        # avg_players = throw_frames.groupby(['game_id', 'play_id']).size().mean()
        # print(f"   DEBUG: Avg players captured per snapshot: {avg_players:.1f} (Should be > 1)")

        # Split into Target vs. Defenders
        targets = throw_frames[throw_frames['player_role'].astype(str).str.strip() == 'Targeted Receiver'][
            ['game_id', 'play_id', 'nfl_id', 'x', 'y', 'week']
        ].rename(columns={'nfl_id': 'target_nfl_id', 'x': 't_x', 'y': 't_y'})

        defenders = throw_frames[throw_frames['player_role'].astype(str).str.strip() == 'Defensive Coverage'][
            ['game_id', 'play_id', 'nfl_id', 'x', 'y']
        ].rename(columns={'nfl_id': 'def_nfl_id', 'x': 'd_x', 'y': 'd_y'})

        # Cartesian Product (Merge)
        merged = defenders.merge(targets, on=['game_id', 'play_id'], how='inner')

        # Calculate Euclidean Distance
        merged['dist'] = np.sqrt(
            (merged['d_x'] - merged['t_x'])**2 + 
            (merged['d_y'] - merged['t_y'])**2
        )

        # Find the Nearest Neighbor
        min_dists = merged.loc[merged.groupby(['game_id', 'play_id'])['dist'].idxmin()]

        context_df = min_dists[['game_id', 'play_id', 'week', 'target_nfl_id', 'def_nfl_id', 'dist']].copy()
        
        context_df = context_df.rename(columns={
            'def_nfl_id': 'nearest_def_nfl_id', 
            'dist': 'dist_at_throw'
        })

        # Apply Classification Labels
        conditions = [
            (context_df['dist_at_throw'] > 5.0),
            (context_df['dist_at_throw'] < 2.0)
        ]
        choices = ['High Void', 'Tight Window']
        context_df['void_type'] = np.select(conditions, choices, default='Neutral')

        return self.output_schema.validate(context_df)
import pandas as pd
import numpy as np
from schema import EraserMetricsSchema

class EraserEngine:
    def __init__(self):
        self.output_schema = EraserMetricsSchema

    def calculate_erasure(self, df: pd.DataFrame, context_df: pd.DataFrame) -> pd.DataFrame:
        """
        PHASE B: The Action.
        Calculates how distinct defenders close space on the targeted receiver.
        """
        # Filter for Post-Throw Phase only
        df_post = df[df['phase'] == 'post_throw'].copy()
        
        # Isolate the Targeted Receiver's path
        # We need the receiver's X,Y for every frame to compare against defenders
        targets = df_post[df_post['player_role'] == 'Targeted Receiver'][
            ['game_id', 'play_id', 'frame_id', 'x', 'y']
        ].rename(columns={'x': 't_x', 'y': 't_y'})

        # Isolate Defenders
        defenders = df_post[df_post['player_role'] == 'Defensive Coverage'][
            ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y']
        ]

        # Merge Defender + Target on (Game, Play, Frame)
        merged = defenders.merge(targets, on=['game_id', 'play_id', 'frame_id'], how='inner')

        # Calculate Dynamic Separation (Distance to Target)
        merged['dist_to_target'] = np.sqrt(
            (merged['x'] - merged['t_x'])**2 + 
            (merged['y'] - merged['t_y'])**2
        )

        def grade_defender(group):
            group = group.sort_values('frame_id')
            
            # A. Get Start and End Distances
            d_start = group['dist_to_target'].iloc[0] # Distance at Throw
            d_end = group['dist_to_target'].iloc[-1]  # Distance at Arrival
            
            # B. Metric 1: VIS (Void Improvement Score)
            # Positive = Good (Closed gap), Negative = Bad (Lost gap)
            vis = d_start - d_end
            
            # C. Metric 2: Closing Speed (Rate of Change)
            # Calculate distance change per frame
            # We multiply by -1 because getting closer (dist going down) is positive speed
            dist_change = group['dist_to_target'].diff() * -1
            
            # Convert to Yards/Second (1 frame = 0.1s)
            speeds = dist_change * 10 
            avg_speed = speeds.mean()
            
            return pd.Series({
                'p_dist_at_throw': d_start,
                'dist_at_arrival': d_end,
                'distance_closed': max(0, vis),
                'vis_score': vis,
                'avg_closing_speed': avg_speed
            })

        # Apply grouping per player per play
        metrics = merged.groupby(['game_id', 'play_id', 'nfl_id']).apply(grade_defender).reset_index()

        return self.output_schema.validate(metrics)
import pandas as pd
import numpy as np
from schema import BenchMarkingSchema, AnalysisReportSchema


# TODO: add more comments on critical parts.
class BenchmarkingEngine:
    def __init__(self):
        self.bench_schema = BenchMarkingSchema
        self.report_schema = AnalysisReportSchema

    def calculate_ceoe(self, df_metrics: pd.DataFrame, 
                       df_context: pd.DataFrame, df_physics: pd.DataFrame) -> pd.DataFrame:

        meta_cols = list(self.bench_schema.to_schema().columns.keys())
        df_meta = self.bench_schema.validate(df_physics[meta_cols])

        df_meta = df_meta.drop_duplicates()

        df_final = df_metrics.merge(df_meta, on=['game_id', 'play_id', 'nfl_id'], how='left')
            
        df_final = df_final.merge(
            df_context[['game_id', 'play_id', 'void_type', 'dist_at_throw']], 
            on=['game_id', 'play_id'], 
            how='left'
        )

        benchmarks = df_final.groupby(['player_position', 'void_type'])['avg_closing_speed'].transform('mean')
        df_final['ceoe_score'] = df_final['avg_closing_speed'] - benchmarks
        df_final['ceoe_score'] = df_final['ceoe_score'].fillna(0.0)
        
        return self.report_schema.validate(df_final)
import os
import pandas as pd
from schema import AnalysisReportSchema, AggregationScoresSchema, FullPlayAnimationSchema

class DataExporter:
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        self.report_schema = AnalysisReportSchema
        self.animation_schema = AggregationScoresSchema
        self.full_animation = FullPlayAnimationSchema

    def export_results(self, df_summary: pd.DataFrame, df_frames: pd.DataFrame):
        """
        PHASE D: EXPORT
        1. Validates & Saves the Analytical Report.
        2. Validates & Merges Scores for Animation.
        3. Saves the Master Animation File.
        """
        print(f"   -> Output Directory: {self.output_dir}")

        # validate report summary
        self.report_schema.validate(df_summary)

        summary_path = os.path.join(self.output_dir, 'eraser_analysis_summary.csv')
        df_summary.to_csv(summary_path, index=False)
        print(f"   -> Saved Eraser Analysis Report to {summary_path}")

        # Define the subset of columns to attach to the visualizer
        score_cols = list(self.animation_schema.to_schema().columns.keys())
        flags_to_merge = self.animation_schema.validate(df_summary[score_cols])

        # MERGE: Left join the scores onto the massive physics dataframe
        # This repeats the score for every frame of the play
        df_animation = df_frames.merge(
            flags_to_merge, 
            on=['game_id', 'play_id', 'nfl_id'], 
            how='left'
        )
        
        # validate all animation data points 
        self.full_animation.validate(df_animation)

        final_path = os.path.join(self.output_dir, 'master_animation_data.csv')
        df_animation.to_csv(final_path, index=False)
        
        print(f"   -> Saved Animation Master File to {final_path}")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.animation as animation
from matplotlib.offsetbox import AnchoredText
from matplotlib.patches import FancyBboxPatch, Circle, Ellipse
import os

class AnimationEngine:
    def __init__(self, summary_path, frames_path, output_dir):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"   [Animator] Loading Data...")
        self.summary_df = pd.read_csv(summary_path)
        
        # ADDED: 'defensive_team', 'yardline_number', 'yardline_side', 'team_coverage_type', 'phase'
        cols = [
            'game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y', 'phase',
            'player_role', 'player_name', 'ball_land_x', 'ball_land_y',
            'down', 'yards_to_go', 'pass_result', 'possession_team', 'defensive_team',
            'yardline_number', 'yardline_side', 'team_coverage_type', 'yards_gained',
            's_derived'  # Speed for display
        ]
        
        self.frames_df = pd.read_csv(frames_path, usecols=cols)

    def _draw_field(self, ax):
        """Sets up the static NFL-style field background."""
        ax.set_xlim(0, 120)
        ax.set_ylim(0, 53.3)
        ax.set_facecolor('#2e7d32')  # NFL Green
        ax.set_aspect('equal')
        ax.axis('off')
        
        # Field outline
        field_rect = patches.Rectangle((10, 0), 100, 53.3, linewidth=2, 
                                         edgecolor='white', facecolor='#3d8b40', zorder=0)
        ax.add_patch(field_rect)
        
        # End Zones
        ez_left = patches.Rectangle((0, 0), 10, 53.3, facecolor='#1b5e20', edgecolor='white', linewidth=2, zorder=0)
        ez_right = patches.Rectangle((110, 0), 10, 53.3, facecolor='#1b5e20', edgecolor='white', linewidth=2, zorder=0)
        ax.add_patch(ez_left)
        ax.add_patch(ez_right)
        ax.text(5, 26.65, 'END\nZONE', ha='center', va='center', fontsize=8, 
                fontweight='bold', color='white', alpha=0.6, rotation=90)
        ax.text(115, 26.65, 'END\nZONE', ha='center', va='center', fontsize=8, 
                fontweight='bold', color='white', alpha=0.6, rotation=270)
        
        # Yard lines (every 5 yards, bold every 10)
        for x in range(10, 111, 5):
            lw = 2 if x % 10 == 0 else 0.5
            alpha = 1.0 if x % 10 == 0 else 0.5
            ax.axvline(x, color='white', linestyle='-', linewidth=lw, alpha=alpha, zorder=1)
        
        # Yard numbers
        for x in range(20, 110, 10):
            num = (x - 10) if x <= 60 else (110 - x)
            # Top numbers
            ax.text(x, 48, str(num), color='white', ha='center', va='center', 
                    fontsize=12, fontweight='bold', alpha=0.8)
            # Bottom numbers
            ax.text(x, 5, str(num), color='white', ha='center', va='center', 
                    fontsize=12, fontweight='bold', alpha=0.8)
        
        # Hash marks (NFL style)
        hash_y_top = 39.3  # 70 feet 9 inches from sideline
        hash_y_bot = 14.0
        for x in range(10, 111, 1):
            ax.plot([x, x], [hash_y_top, hash_y_top + 0.5], color='white', linewidth=0.5, alpha=0.6, zorder=1)
            ax.plot([x, x], [hash_y_bot - 0.5, hash_y_bot], color='white', linewidth=0.5, alpha=0.6, zorder=1)

    def generate_video(self, game_id, play_id, eraser_id, filename="play_animation.mp4"):
        print(f"   [Animator] Rendering video for {game_id}-{play_id}...")
        
        # 1. Get Play Data
        play_frames = self.frames_df[
            (self.frames_df['game_id'] == game_id) & 
            (self.frames_df['play_id'] == play_id)
        ].sort_values('frame_id')
        
        if play_frames.empty: return

        # 2. Actors & Context
        target_row = play_frames[play_frames['player_role'] == 'Targeted Receiver']
        target_id = target_row['nfl_id'].iloc[0]
        target_name = target_row['player_name'].iloc[0] if 'player_name' in target_row.columns else "WR"
        
        # Get QB info
        qb_row = play_frames[play_frames['player_role'] == 'Passer']
        qb_id = qb_row['nfl_id'].iloc[0] if not qb_row.empty else None
        qb_name = qb_row['player_name'].iloc[0] if not qb_row.empty and 'player_name' in qb_row.columns else "QB"
        
        summary_row = self.summary_df[
            (self.summary_df['game_id'] == game_id) & 
            (self.summary_df['play_id'] == play_id)
        ]
        
        # Get Scores & Names
        vis_score = 0.0
        start_dist = 0.0
        end_dist = 0.0
        context_id = None
        eraser_name = "DEF"
        context_name = "DEF"
        
        if not summary_row.empty:
            eraser_row = summary_row[summary_row['nfl_id'] == eraser_id]
            if not eraser_row.empty:
                vis_score = eraser_row.iloc[0]['vis_score']
                start_dist = eraser_row.iloc[0]['p_dist_at_throw']
                end_dist = eraser_row.iloc[0].get('p_dist_at_arrival', start_dist - vis_score)
                if 'player_name' in eraser_row.columns:
                    eraser_name = eraser_row.iloc[0]['player_name']
            # Get context defender (closest at throw)
            context_idx = summary_row['p_dist_at_throw'].idxmin()
            context_id = summary_row.loc[context_idx]['nfl_id']
            if 'player_name' in summary_row.columns:
                context_name = summary_row.loc[context_idx]['player_name']

        # Get play metadata
        meta = play_frames.iloc[0]
        pass_result = meta.get('pass_result', 'Unknown')
        yards_gained = meta.get('yards_gained', 0)
        
        # 3. BALL TRAJECTORY LOGIC
        unique_frames = sorted(play_frames['frame_id'].unique())
        start_frame = play_frames['frame_id'].min()
        end_frame = play_frames['frame_id'].max()
        
        # Identify the throw frame (first frame of post_throw phase)
        if 'phase' in play_frames.columns:
            post_throw_data = play_frames[play_frames['phase'] == 'post_throw']
            if not post_throw_data.empty:
                throw_frame = post_throw_data['frame_id'].min()
            else:
                throw_frame = start_frame
        else:
            throw_frame = start_frame
        
        # Get post-throw frames for ball interpolation
        post_throw_frames = sorted([f for f in unique_frames if f >= throw_frame])
        post_throw_steps = len(post_throw_frames)
        
        # Ball landing position
        bx_end, by_end = play_frames.iloc[0]['ball_land_x'], play_frames.iloc[0]['ball_land_y']
        
        # Get QB position at throw frame
        passer_at_throw = play_frames[
            (play_frames['frame_id'] == throw_frame) & 
            (play_frames['player_role'] == 'Passer')
        ]
        if not passer_at_throw.empty:
            bx_throw, by_throw = passer_at_throw.iloc[0]['x'], passer_at_throw.iloc[0]['y']
        else:
            passer_early = play_frames[play_frames['player_role'] == 'Passer'].sort_values('frame_id')
            if not passer_early.empty:
                bx_throw, by_throw = passer_early.iloc[0]['x'], passer_early.iloc[0]['y']
            else:
                bx_throw, by_throw = bx_end - 15, 26.65
        
        # Interpolate ball trajectory ONLY for post-throw frames
        if post_throw_steps > 1:
            ball_x_flight = np.linspace(bx_throw, bx_end, post_throw_steps)
            ball_y_flight = np.linspace(by_throw, by_end, post_throw_steps)
        else:
            ball_x_flight = [bx_end]
            ball_y_flight = [by_end]
        
        # Build ball position dictionary
        ball_pos_dict = {}
        for f in unique_frames:
            if f < throw_frame:
                passer_at_f = play_frames[
                    (play_frames['frame_id'] == f) & 
                    (play_frames['player_role'] == 'Passer')
                ]
                if not passer_at_f.empty:
                    ball_pos_dict[f] = (passer_at_f.iloc[0]['x'], passer_at_f.iloc[0]['y'])
                else:
                    ball_pos_dict[f] = (bx_throw, by_throw)
            else:
                idx = post_throw_frames.index(f)
                ball_pos_dict[f] = (ball_x_flight[idx], ball_y_flight[idx])

        # ===== BUILD PLAYER POSITION CACHE =====
        # Cache last known position for each player to handle "ghost" players
        # who disappear in post_throw frames (>8yds from catch point in output data)
        all_player_ids = play_frames['nfl_id'].unique()
        player_cache = {}  # {nfl_id: {'x': x, 'y': y, 'role': role, 's_derived': speed}}
        
        # Pre-build cache by iterating through frames in order
        player_positions_by_frame = {}
        for f in unique_frames:
            snap = play_frames[play_frames['frame_id'] == f]
            frame_positions = {}
            
            for pid in all_player_ids:
                player_row = snap[snap['nfl_id'] == pid]
                if not player_row.empty:
                    # Update cache with current position
                    player_cache[pid] = {
                        'x': player_row.iloc[0]['x'],
                        'y': player_row.iloc[0]['y'],
                        'role': player_row.iloc[0]['player_role'],
                        's_derived': player_row.iloc[0].get('s_derived', 0)
                    }
                # Use cached position (current or last known)
                if pid in player_cache:
                    frame_positions[pid] = player_cache[pid].copy()
            
            player_positions_by_frame[f] = frame_positions

        # 4. Setup Figure
        fig, ax = plt.subplots(figsize=(14, 7))
        self._draw_field(ax)
        
        # ===== INFO PANELS =====
        
        # Context Box (Top Left) - Down & Distance, Coverage
        yd_str = f"{meta['yardline_side']} {int(meta['yardline_number'])}"
        cov_str = str(meta['team_coverage_type']).replace('_', ' ').title()
        context_text = f"{int(meta['down'])} & {int(meta['yards_to_go'])} | {yd_str}\n{cov_str}"
        at_context = AnchoredText(context_text, loc='upper left', 
                                   prop=dict(size=11, fontweight='bold', family='monospace'), frameon=True)
        at_context.patch.set_boxstyle("round,pad=0.4")
        at_context.patch.set_facecolor('#1a1a2e')
        at_context.patch.set_edgecolor('white')
        at_context.patch.set_alpha(0.95)
        for txt in at_context.txt.get_children():
            txt.set_color('white')
        ax.add_artist(at_context)

        # Metric Box (Top Center) - VIS Score
        sign = "+" if vis_score > 0 else ""
        metric_text = f"START: {start_dist:.1f} yds\nVIS: {sign}{vis_score:.1f} yds"
        at_metric = AnchoredText(metric_text, loc='upper center', 
                                  prop=dict(size=12, fontweight='bold', family='monospace'), frameon=True)
        at_metric.patch.set_boxstyle("round,pad=0.4")
        at_metric.patch.set_facecolor('#16213e')
        at_metric.patch.set_edgecolor('#00ff88' if vis_score > 0 else '#ff4444')
        at_metric.patch.set_linewidth(3)
        for txt in at_metric.txt.get_children():
            txt.set_color('#00ff88' if vis_score > 0 else '#ff4444')
        ax.add_artist(at_metric)

        # Outcome Badge (Top Right area) - shows COMPLETE/INCOMPLETE
        outcome_color = '#27ae60' if pass_result == 'C' else '#c0392b'
        outcome_text = 'COMPLETE ‚úì' if pass_result == 'C' else 'INCOMPLETE ‚úó'
        outcome_label = ax.text(118, 50, outcome_text, ha='right', va='top', fontsize=11, 
                                 fontweight='bold', color='white',
                                 bbox=dict(facecolor=outcome_color, edgecolor='white', 
                                          pad=6, boxstyle='round,pad=0.4'))

        # Timer (Right side)
        timer_text = ax.text(118, 42, '', ha='right', fontsize=13, fontweight='bold', color='white',
                             bbox=dict(facecolor='#2c3e50', edgecolor='white', pad=4, boxstyle='round,pad=0.3'))
        
        # Phase Label
        phase_label = ax.text(118, 35, '', ha='right', fontsize=11, fontweight='bold', color='white',
                              bbox=dict(facecolor='#3498db', edgecolor='white', pad=5, boxstyle='round,pad=0.3'))
        
        # Speed indicator for eraser
        speed_label = ax.text(118, 28, '', ha='right', fontsize=10, fontweight='bold', color='white',
                              bbox=dict(facecolor='#8e44ad', edgecolor='white', pad=4, boxstyle='round,pad=0.3'))

        # ===== STATIC MARKERS =====
        
        # Ball Landing Spot (X marker - visible entire animation)
        ax.scatter([bx_end], [by_end], c='#ffff00', s=400, marker='X', 
                   edgecolors='#ff6600', linewidths=3, zorder=2, alpha=0.9)
        ax.text(bx_end, by_end - 2.5, 'TARGET', ha='center', va='top', fontsize=7,
                fontweight='bold', color='#ffff00', alpha=0.9)
        
        # ===== PLAYER MARKERS =====
        
        # Other defenders (blue circles)
        scat_def_others = ax.scatter([], [], c='#2980b9', s=200, marker='o', 
                                      edgecolors='white', linewidths=2, zorder=3, alpha=0.7)
        
        # Other offense (red circles)  
        scat_off_others = ax.scatter([], [], c='#c0392b', s=200, marker='o',
                                      edgecolors='white', linewidths=2, zorder=3, alpha=0.7)
        
        # QB (purple diamond - distinctive)
        scat_qb = ax.scatter([], [], c='#9b59b6', s=350, marker='D', 
                              edgecolors='white', linewidths=3, zorder=5)
        qb_label = ax.text(0, 0, '', ha='center', va='bottom', fontsize=8,
                           fontweight='bold', color='white',
                           bbox=dict(facecolor='#9b59b6', edgecolor='none', pad=2, alpha=0.9))
        
        # Target Receiver (orange star - stands out)
        scat_target = ax.scatter([], [], c='#ff6b35', s=450, marker='*', 
                                  edgecolors='white', linewidths=2, zorder=6)
        target_label = ax.text(0, 0, '', ha='center', va='bottom', fontsize=8, 
                               fontweight='bold', color='white',
                               bbox=dict(facecolor='#ff6b35', edgecolor='none', pad=2, alpha=0.9))
        
        # Eraser (bright green, largest circle)
        scat_eraser = ax.scatter([], [], c='#00ff88', s=400, marker='o',
                                  edgecolors='#004d40', linewidths=3, zorder=7)
        eraser_label = ax.text(0, 0, '', ha='center', va='bottom', fontsize=8,
                               fontweight='bold', color='black',
                               bbox=dict(facecolor='#00ff88', edgecolor='none', pad=2, alpha=0.9))
        
        # Context Defender (cyan triangle - closest at throw)
        scat_context = ax.scatter([], [], c='#00bcd4', s=300, marker='^',
                                   edgecolors='white', linewidths=2, zorder=5)
        context_label = ax.text(0, 0, '', ha='center', va='bottom', fontsize=8,
                                fontweight='bold', color='white',
                                bbox=dict(facecolor='#00bcd4', edgecolor='none', pad=2, alpha=0.9))
        
        # Ball (brown pentagon - football shape)
        scat_ball = ax.scatter([], [], c='#8B4513', s=200, marker='p',
                                edgecolors='white', linewidths=2, zorder=10)
        
        # Void line (dashed line between eraser and target)
        line_void, = ax.plot([], [], color='#ffff00', linestyle='--', linewidth=2, alpha=0.8, zorder=4)
        text_void = ax.text(0, 0, '', ha='center', va='center', fontsize=10, fontweight='bold', 
                            color='black', bbox=dict(facecolor='#ffff00', alpha=0.9, edgecolor='none', pad=3))

        # ===== LEGEND =====
        legend_elements = [
            plt.scatter([], [], c='#00ff88', s=150, marker='o', edgecolors='#004d40', linewidths=2, label='Eraser'),
            plt.scatter([], [], c='#ff6b35', s=150, marker='*', edgecolors='white', linewidths=2, label='Target WR'),
            plt.scatter([], [], c='#00bcd4', s=100, marker='^', edgecolors='white', linewidths=2, label='Nearest Def'),
            plt.scatter([], [], c='#9b59b6', s=100, marker='D', edgecolors='white', linewidths=2, label='QB'),
            plt.scatter([], [], c='#8B4513', s=80, marker='p', edgecolors='white', linewidths=2, label='Ball'),
            plt.scatter([], [], c='#ffff00', s=100, marker='X', edgecolors='#ff6600', linewidths=2, label='Ball Target'),
        ]
        ax.legend(handles=legend_elements, loc='lower right', fontsize=8, 
                  framealpha=0.95, facecolor='#1a1a2e', edgecolor='white', labelcolor='white',
                  ncol=2)

        # 6. Update Loop
        def update(frame_num):
            # Timer relative to throw frame
            time_sec = (frame_num - throw_frame) * 0.1
            if frame_num < throw_frame:
                timer_text.set_text(f"T {time_sec:.1f}s")
                phase_label.set_text("‚è≥ PRE THROW")
                phase_label.get_bbox_patch().set_facecolor('#3498db')
            else:
                timer_text.set_text(f"T +{time_sec:.1f}s")
                phase_label.set_text("üèà BALL IN AIR")
                phase_label.get_bbox_patch().set_facecolor('#e74c3c')

            # Get cached positions for this frame (includes frozen players)
            frame_positions = player_positions_by_frame.get(frame_num, {})
            
            # Ball position
            if frame_num in ball_pos_dict:
                scat_ball.set_offsets([ball_pos_dict[frame_num]])
            
            # Build position lists from cache
            def_others_pos = []
            off_others_pos = []
            
            excluded_ids = {eraser_id, target_id, context_id}
            if qb_id:
                excluded_ids.add(qb_id)
            
            for pid, pos in frame_positions.items():
                if pid in excluded_ids:
                    continue
                role = pos.get('role', '')
                if role in ['Coverage Defender', 'Pass Rusher', 'Defender']:
                    def_others_pos.append([pos['x'], pos['y']])
                elif role not in ['Passer']:
                    off_others_pos.append([pos['x'], pos['y']])
            
            scat_def_others.set_offsets(np.array(def_others_pos) if def_others_pos else np.empty((0, 2)))
            scat_off_others.set_offsets(np.array(off_others_pos) if off_others_pos else np.empty((0, 2)))
            
            # QB (from cache)
            if qb_id and qb_id in frame_positions:
                qb_pos = frame_positions[qb_id]
                qx, qy = qb_pos['x'], qb_pos['y']
                scat_qb.set_offsets([[qx, qy]])
                qb_label.set_position((qx, qy + 2.5))
                q_name = str(qb_name).split()[-1][:8] if qb_name else "QB"
                qb_label.set_text(q_name)
            else:
                scat_qb.set_offsets(np.empty((0, 2)))
            
            # Target (from cache)
            if target_id in frame_positions:
                target_pos = frame_positions[target_id]
                tx, ty = target_pos['x'], target_pos['y']
                scat_target.set_offsets([[tx, ty]])
                target_label.set_position((tx, ty + 2.5))
                t_name = str(target_name).split()[-1][:8] if target_name else "WR"
                target_label.set_text(t_name)
            
            # Context defender (from cache, with name label)
            if context_id and context_id in frame_positions and context_id != eraser_id: 
                context_pos = frame_positions[context_id]
                cx, cy = context_pos['x'], context_pos['y']
                scat_context.set_offsets([[cx, cy]])
                context_label.set_position((cx, cy + 2.5))
                c_name = str(context_name).split()[-1][:8] if context_name else "DEF"
                context_label.set_text(c_name)
            else: 
                scat_context.set_offsets(np.empty((0, 2)))
                context_label.set_text('')
                
            # Eraser (from cache)
            if eraser_id in frame_positions:
                eraser_pos = frame_positions[eraser_id]
                ex, ey = eraser_pos['x'], eraser_pos['y']
                scat_eraser.set_offsets([[ex, ey]])
                eraser_label.set_position((ex, ey + 2.5))
                e_name = str(eraser_name).split()[-1][:8] if eraser_name else "DEF"
                eraser_label.set_text(e_name)
                
                # Speed display
                speed_yps = eraser_pos.get('s_derived', 0)
                speed_mph = speed_yps * 2.045 if pd.notna(speed_yps) else 0
                speed_label.set_text(f"‚ö° {speed_mph:.1f} mph")
                
                # Void line
                if target_id in frame_positions:
                    line_void.set_data([ex, tx], [ey, ty])
                    dist = np.sqrt((ex-tx)**2 + (ey-ty)**2)
                    text_void.set_position(((ex+tx)/2, (ey+ty)/2))
                    text_void.set_text(f"{dist:.1f} yds")
            
            return (scat_def_others, scat_off_others, scat_target, scat_eraser, scat_context, 
                    scat_ball, scat_qb, line_void, text_void, timer_text, phase_label, speed_label,
                    target_label, eraser_label, qb_label, context_label)

        # 7. Render
        print("      ... Rendering frames ...")
        ani = animation.FuncAnimation(fig, update, frames=unique_frames, interval=100, blit=True)
        
        save_path = os.path.join(self.output_dir, filename)
        ani.save(save_path, writer='pillow', fps=10)
        print(f"   -> Saved Video: {save_path}")
        plt.close()
import pandas as pd
import numpy as np

class StoryDataEngine:
    def __init__(self, summary_path: str, frames_path: str, seed=42):
        self.summary_df = pd.read_csv(summary_path)
        self.frames_path = frames_path
        self.seed = seed
        
    def cast_archetypes(self):
        """
        Scans summary for archetypes. Samples from top candidates to avoid outliers.
        """
        df = self.summary_df
        
        # Deep start (>10), High VIS (>4). Best = Highest VIS.
        eraser_pool = df[
            (df['p_dist_at_throw'] > 10) & 
            (df['vis_score'] > 4) & 
            (df['dist_at_arrival'] < 3) 
        ]
        eraser = self._select_candidate(eraser_pool, sort_col='vis_score', ascending=False)
        
        # Tight start (<2.5), Tight finish (<1.5). Best = Lowest Arrival Dist.
        lockdown_pool = df[
            (df['p_dist_at_throw'] < 2.5) & 
            (df['dist_at_arrival'] < 1.5) &
            (df['vis_score'].abs() < 2)
        ]
        lockdown = self._select_candidate(lockdown_pool, sort_col='dist_at_arrival', ascending=True)
        
        # Cushion (>5), Bad finish (VIS <-4). Best = Lowest (Negative) VIS.
        liability_pool = df[
            (df['p_dist_at_throw'] > 5) & 
            (df['vis_score'] < -4) &
            (df['dist_at_arrival'] > 8)
        ]
        liability = self._select_candidate(liability_pool, sort_col='vis_score', ascending=True)
        
        # Tight start (<3), Bad finish (VIS <-3). Best = Lowest (Negative) VIS.
        lost_step_pool = df[
            (df['p_dist_at_throw'] < 3) & 
            (df['vis_score'] < -3) &
            (df['dist_at_arrival'] > 6)
        ]
        lost_step = self._select_candidate(lost_step_pool, sort_col='vis_score', ascending=True)

        return {
            'Eraser': self._extract_meta(eraser, "Top Eraser (FS)"),
            'Lockdown': self._extract_meta(lockdown, "Lockdown (CB)"),
            'Liability': self._extract_meta(liability, "Liability (Busted)"),
            'Lost Step': self._extract_meta(lost_step, "Lost Step (Double Move)")
        }

    def _select_candidate(self, df, sort_col, ascending, top_n=5):
        """
        Sorts by criteria, takes top N, then randomly picks one.
        """
        if df.empty: return pd.DataFrame()
        
        # Sort to find best candidates
        sorted_df = df.sort_values(sort_col, ascending=ascending)
        
        # Take top chunk (to ensure quality)
        candidates = sorted_df.head(top_n)
        
        # Sample one (to avoid outlier dependence)
        return candidates.sample(n=1, random_state=self.seed)

    def _extract_meta(self, row, label):
        if row.empty: return None
        return {
            'game_id': int(row.iloc[0]['game_id']),
            'play_id': int(row.iloc[0]['play_id']),
            'nfl_id': float(row.iloc[0]['nfl_id']),
            'vis_score': float(row.iloc[0]['vis_score']),
            'label': label
        }

    def get_play_frames(self, play_meta):
        if not play_meta: return pd.DataFrame()
        df = pd.read_csv(self.frames_path)
        return df[(df['game_id'] == play_meta['game_id']) & (df['play_id'] == play_meta['play_id'])].copy()
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import matplotlib.patches as patches
from scipy.interpolate import UnivariateSpline

# Set style for professional report visuals
sns.set_theme(style="whitegrid", context="talk")
plt.rcParams['font.family'] = 'sans-serif'

class StoryVisualEngine:
    def __init__(self, summary_path: str, animation_path: str, output_dir: str):
        print(f"   [VizGen] Loading Summary: {summary_path}...")
        self.summary_df = pd.read_csv(summary_path)
        
        print(f"   [VizGen] Loading Animation Data (Lazy): {animation_path}...")
        # We load specific columns to keep memory low
        req_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'player_role', 'x', 'y']
        self.frames_df = pd.read_csv(animation_path, usecols=req_cols)
        
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Standardize colors
        self.quad_colors = {
            'Eraser': '#2ecc71',      # Green
            'Lockdown': '#3498db',    # Blue
            'Lost Step': '#f1c40f',   # Yellow
            'Liability': '#e74c3c',   # Red
            'Neutral': '#95a5a6'
        }

    # =========================================
    # VISUAL 1: ERASER LANDSCAPE (SCATTER)
    # =========================================
    def plot_eraser_landscape(self, cast_dict):
        """
        Plots S_throw vs S_arrival.
        Annotates the specific plays identified by StoryEngine.
        """
        print("   [VizGen] Generating V1: Eraser Landscape...")
        df = self.summary_df.copy()
        
        # Apply Quadrant Logic for Coloring
        conditions = [
            (df['p_dist_at_throw'] >= 3.0) & (df['dist_at_arrival'] <= 1.5), # Eraser
            (df['p_dist_at_throw'] < 3.0) & (df['dist_at_arrival'] <= 1.5),  # Lockdown
            (df['p_dist_at_throw'] < 3.0) & (df['dist_at_arrival'] > 1.5),   # Lost Step
            (df['p_dist_at_throw'] >= 3.0) & (df['dist_at_arrival'] > 1.5)   # Liability
        ]
        choices = ['Eraser', 'Lockdown', 'Lost Step', 'Liability']
        df['quadrant_plot'] = np.select(conditions, choices, default='Neutral')
        
        fig, ax = plt.subplots(figsize=(12, 12))
        
        # 1. Main Scatter
        sns.scatterplot(
            data=df, x='p_dist_at_throw', y='dist_at_arrival',
            hue='quadrant_plot', palette=self.quad_colors,
            alpha=0.6, s=40, linewidth=0, ax=ax
        )
        
        # 2. Reference Lines
        ax.plot([0, 25], [0, 25], 'k--', lw=2, alpha=0.3, label='Break Even (VIS=0)')
        ax.axvline(x=3.0, color='gray', linestyle=':', lw=2)
        ax.axhline(y=1.5, color='gray', linestyle=':', lw=2)

        # 3. Annotations (Driven by StoryEngine)
        # We iterate through the 'Cast' dictionary
        for role, play_meta in cast_dict.items():
            if play_meta is None: continue
            
            # Find coordinates in summary
            row = df[(df['game_id'] == play_meta['game_id']) & 
                     (df['play_id'] == play_meta['play_id']) & 
                     (df['nfl_id'] == play_meta['nfl_id'])]
            
            if not row.empty:
                sx = row.iloc[0]['p_dist_at_throw']
                sy = row.iloc[0]['dist_at_arrival']
                
                # Annotate
                ax.annotate(play_meta['label'], 
                            (sx, sy), xytext=(sx+2, sy+2),
                            arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=8),
                            fontsize=11, fontweight='bold', 
                            bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="black", lw=1))

        ax.set_title('The Eraser Landscape: Recovery vs. Result', fontsize=18, fontweight='bold', pad=20)
        ax.set_xlabel('Player Distance at Throw (The Mess)', fontsize=14, fontweight='bold')
        ax.set_ylabel('Distance at Arrival (The Finish)', fontsize=14, fontweight='bold')
        ax.set_xlim(0, 20)
        ax.set_ylim(0, 20)
        ax.legend(title='Quadrants', loc='upper right')
        
        output_path = os.path.join(self.output_dir, 'V1_Eraser_Landscape.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

    # =========================================
    # VISUAL 2: RACE CHARTS (TRAJECTORIES)
    # =========================================
    def plot_race_charts(self, cast_dict):
        """
        Plots the distance-over-time for the 4 archetypes selected by StoryEngine.
        """
        print("   [VizGen] Generating V2: Race Charts...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # Removed sharey=True for independent axes
        axes = axes.flatten()
        quad_order = ['Eraser', 'Lockdown', 'Liability', 'Lost Step']

        # Legend Elements
        legend_elements = [
            plt.Line2D([0], [0], color='grey', lw=3, label='Defender Path'),
            plt.Line2D([0], [0], color='gray', linestyle=':', label='Closed (1.5y)'),
            plt.Line2D([0], [0], marker='o', color='grey', label='Throw', markersize=10, linestyle='None'),
            plt.Line2D([0], [0], marker='X', color='grey', label='Arrival', markersize=10, linestyle='None')
        ]

        for i, quad_name in enumerate(quad_order):
            ax = axes[i]
            play_meta = cast_dict.get(quad_name)
            color = self.quad_colors.get(quad_name, 'grey')

            # Handle Missing Cast Member
            if play_meta is None:
                ax.set_title(f"{quad_name}", fontsize=16, fontweight='bold', color=color)
                ax.text(0.5, 0.5, "No Candidate Found", ha='center')
                continue

            # Title with VIS Score
            vis_val = play_meta['vis_score']
            sign = "+" if vis_val > 0 else ""
            ax.set_title(f"{quad_name}\n(VIS: {sign}{vis_val:.1f} yds)", fontsize=16, fontweight='bold', color=color)

            # Get Tracking Data
            play_df = self.frames_df[
                (self.frames_df['game_id'] == play_meta['game_id']) & 
                (self.frames_df['play_id'] == play_meta['play_id'])
            ]
            
            def_track = play_df[play_df['nfl_id'] == play_meta['nfl_id']].sort_values('frame_id')
            target_track = play_df[play_df['player_role'] == 'Targeted Receiver'].sort_values('frame_id')

            if def_track.empty or target_track.empty:
                continue
                    
            merged = pd.merge(def_track, target_track, on='frame_id', suffixes=('_d', '_t'))
            merged['dist'] = np.sqrt((merged['x_d'] - merged['x_t'])**2 + (merged['y_d'] - merged['y_t'])**2)
            merged['time_sec'] = (merged['frame_id'] - merged['frame_id'].min()) * 0.1

            # Apply spline smoothing to reduce tracking noise (s=50 recommended balance)
            time_arr = merged['time_sec'].values
            dist_arr = merged['dist'].values
            
            # Spline needs at least 4 points and unique x values
            if len(time_arr) >= 4:
                try:
                    spline = UnivariateSpline(time_arr, dist_arr, s=50)
                    smooth_dist = spline(time_arr)
                except:
                    smooth_dist = dist_arr  # Fallback to raw if spline fails
            else:
                smooth_dist = dist_arr
            
            merged['smooth_dist'] = smooth_dist
            
            # Calculate axis limits early (needed for annotations)
            max_dist = merged['smooth_dist'].max()
            max_time = merged['time_sec'].max()

            # PLOT LOGIC
            # 1. Contested Zone Shading (0-1.5 yards) - light gray background
            ax.axhspan(0, 1.5, color='#d5d5d5', alpha=0.4, zorder=0)
            ax.text(max_time * 0.95, 0.75, 'CONTESTED\nZONE', ha='right', va='center',
                   fontsize=8, color='#666666', style='italic', alpha=0.8)
            
            # 2. Smoothed Line & Fill
            ax.plot(merged['time_sec'], merged['smooth_dist'], lw=4, color=color, alpha=0.9)
            ax.fill_between(merged['time_sec'], merged['smooth_dist'], 0, color=color, alpha=0.1)

            # 3. Markers (use smoothed values for consistency)
            start = merged.iloc[0]
            end = merged.iloc[-1]
            
            ax.scatter(start['time_sec'], start['smooth_dist'], color=color, s=150, marker='o', zorder=5, edgecolors='white', lw=2)
            ax.annotate('THROW', (start['time_sec'], start['smooth_dist']), xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold', color=color)

            ax.scatter(end['time_sec'], end['smooth_dist'], color=color, s=150, marker='X', zorder=5, edgecolors='white', lw=2)
            ax.annotate('ARRIVAL', (end['time_sec'], end['smooth_dist']), xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold', color=color)

            # 4. Closing Rate annotation (yards closed per second)
            flight_time = end['time_sec'] - start['time_sec']
            closing_rate = (start['smooth_dist'] - end['smooth_dist']) / flight_time if flight_time > 0 else 0
            rate_sign = "+" if closing_rate < 0 else "‚àí"  # Negative = opening up, Positive = closing
            
            # Position rate annotation at midpoint of trajectory
            mid_idx = len(merged) // 2
            mid_time = merged.iloc[mid_idx]['time_sec']
            mid_dist = merged.iloc[mid_idx]['smooth_dist']
            
            if quad_name in ['Eraser', 'Lockdown']:
                # Show closing rate for closers
                ax.annotate(f'{rate_sign}{abs(closing_rate):.1f} yds/sec', 
                           (mid_time, mid_dist), xytext=(10, -15), textcoords='offset points',
                           fontsize=10, fontweight='bold', color=color,
                           bbox=dict(boxstyle='round,pad=0.2', facecolor='white', edgecolor=color, alpha=0.8),
                           arrowprops=dict(arrowstyle='->', color=color, lw=1.5))
            
            # 5. Decision Point markers for Liability & Lost Step (find inflection point)
            if quad_name in ['Liability', 'Lost Step']:
                # Find the minimum point (where trajectory changes direction)
                smooth_arr = merged['smooth_dist'].values
                min_idx = np.argmin(smooth_arr)
                
                # Only mark if it's not at the start or end (true inflection)
                if 2 < min_idx < len(smooth_arr) - 2:
                    inflection_time = merged.iloc[min_idx]['time_sec']
                    inflection_dist = smooth_arr[min_idx]
                    
                    # Decision point marker
                    ax.scatter(inflection_time, inflection_dist, color='#e67e22', s=200, 
                              marker='o', zorder=6, edgecolors='white', lw=2)
                    
                    label = "Lost leverage" if quad_name == 'Liability' else "Lost balance"
                    ax.annotate(f'‚ö† {label}', (inflection_time, inflection_dist), 
                               xytext=(8, 12), textcoords='offset points',
                               fontsize=9, fontweight='bold', color='#c0392b',
                               bbox=dict(boxstyle='round,pad=0.2', facecolor='#fdebd0', edgecolor='#e67e22', alpha=0.9))
            
            # Store flight time for bottom annotation
            play_meta['flight_time'] = flight_time
            
            # Formatting with dynamic limits
            ax.axhline(1.5, color='gray', linestyle=':', lw=2)
            ax.axhline(0, color='black', lw=1)
            ax.set_ylim(-0.5, max(max_dist * 1.15, 5))  # 15% padding, minimum 5 yards
            ax.set_xlim(0, max_time + 0.3)  # Small padding on x-axis
            ax.grid(True, alpha=0.3)
            
            if i in [2, 3]: ax.set_xlabel('Seconds After Throw', fontsize=12, fontweight='bold')
            if i in [0, 2]: ax.set_ylabel('Separation (Yards)', fontsize=12, fontweight='bold')

        # Global Legend
        fig.legend(handles=legend_elements, loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.02), frameon=False, fontsize=11)
        
        # Flight Time annotation at bottom
        flight_parts = []
        for quad_name in quad_order:
            meta = cast_dict.get(quad_name)
            if meta and 'flight_time' in meta:
                flight_parts.append(f"{quad_name}: {meta['flight_time']:.1f}s")
        
        if flight_parts:
            flight_text = "Ball Flight Time ‚Äî " + " | ".join(flight_parts)
            fig.text(0.5, -0.06, flight_text, ha='center', va='top', fontsize=10, 
                    color='#555555', style='italic')
        
        plt.tight_layout(rect=[0, 0.02, 1, 1])  # Leave room for bottom annotations
        
        output_path = os.path.join(self.output_dir, 'V2_Race_Charts.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

    # =========================================
    # VISUAL 3: HEATMAP
    # =========================================
    def plot_coverage_heatmap(self):
        """
        Average VIS by Route Depth vs Coverage Type.
        """
        print("   [VizGen] Generating V3: Coverage Heatmap...")
        df = self.summary_df.copy()

        if 'pass_length' not in df.columns or 'team_coverage_type' not in df.columns:
            print("      [!] Skipping Heatmap: Columns missing.")
            return

        # Binning
        bins = [-5, 5, 10, 15, 25, 100]
        labels = ['Behind LOS', 'Short (0-5)', 'Medium (5-10)', 'Int (10-15)', 'Deep (15+)']
        df['depth_band'] = pd.cut(df['pass_length'], bins=bins, labels=labels)

        # Filter
        main_coverages = ['COVER_1', 'COVER_2_MAN', 'COVER_2_ZONE', 'COVER_3_ZONE', 'COVER_4_ZONE', 'COVER_6_ZONE']
        df = df[df['team_coverage_type'].isin(main_coverages)]

        # Pivot
        grouped = df.groupby(['team_coverage_type', 'depth_band'], observed=False)
        heatmap_data = grouped['vis_score'].mean()
        counts = grouped.size()
        heatmap_data = heatmap_data.where(counts >= 10).unstack()

        # Plot
        fig, ax = plt.subplots(figsize=(12, 8))
        cmap = sns.diverging_palette(10, 130, as_cmap=True) # Red-Green
        
        sns.heatmap(
            heatmap_data, annot=True, fmt=".1f", cmap=cmap,
            center=0, vmin=-2, vmax=4, linewidths=.5,
            cbar_kws={'label': 'Average VIS (Yards Erased)'}, ax=ax
        )

        ax.set_title('Where Defenses Erase Space: Scheme vs. Depth', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('Target Depth (Air Yards)', fontsize=14)
        ax.set_ylabel('Coverage Scheme', fontsize=14)
        plt.xticks(rotation=45, ha='right')
        
        output_path = os.path.join(self.output_dir, 'V3_Coverage_Heatmap.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

    # =========================================
    # VISUAL 4: EPA/YAC IMPACT CHART
    # =========================================
    def plot_effort_impact_chart(self):
        """
        Slope chart showing Effort ‚Üí Outcome (EPA/YAC saved).
        Ties the story together: Context ‚Üí Effort ‚Üí Result.
        """
        print("   [VizGen] Generating V4: Effort Impact Chart...")
        df = self.summary_df.copy()
        
        # 1. Filter for Completions Only (where damage occurs)
        completed = df[df['pass_result'] == 'C'].copy()
        
        if completed.empty:
            print("      [!] Skipping Impact Chart: No completions found.")
            return
        
        # 2. Derive YAC
        completed['yac'] = completed['yards_gained'] - completed['pass_length']
        
        # 3. Create Start Distance bands
        dist_bins = [0, 3, 6, 10, 100]
        dist_labels = ['Tight\n(0-3 yds)', 'Medium\n(3-6 yds)', 'High Void\n(6-10 yds)', 'Exempt\n(10+ yds)']
        completed['start_band'] = pd.cut(completed['p_dist_at_throw'], bins=dist_bins, labels=dist_labels)
        
        # 4. Calculate VIS quartiles WITHIN each start band
        def get_quartile_label(group):
            q25 = group['vis_score'].quantile(0.25)
            q75 = group['vis_score'].quantile(0.75)
            conditions = [
                group['vis_score'] <= q25,
                group['vis_score'] >= q75
            ]
            choices = ['Low Effort', 'High Effort']
            group['effort_bucket'] = np.select(conditions, choices, default='Middle')
            return group
        
        completed = completed.groupby('start_band', group_keys=False, observed=False).apply(get_quartile_label)
        
        # 5. Filter to only Q1 and Q4 for clean comparison
        extremes = completed[completed['effort_bucket'].isin(['Low Effort', 'High Effort'])]
        
        # 6. Aggregate by band and effort
        impact_data = extremes.groupby(['start_band', 'effort_bucket'], observed=False).agg(
            avg_epa=('expected_points_added', 'mean'),
            avg_yac=('yac', 'mean'),
            count=('play_id', 'count')
        ).reset_index()
        
        # 7. Create the figure with two subplots (EPA and YAC)
        fig, axes = plt.subplots(1, 2, figsize=(16, 8))
        
        bands = ['Tight\n(0-3 yds)', 'Medium\n(3-6 yds)', 'High Void\n(6-10 yds)', 'Exempt\n(10+ yds)']
        x_positions = np.arange(len(bands))
        
        for ax_idx, (metric, title, ylabel) in enumerate([
            ('avg_epa', 'EPA Impact: Effort Saves Points', 'Expected Points Added'),
            ('avg_yac', 'YAC Impact: Effort Limits Damage', 'Yards After Catch')
        ]):
            ax = axes[ax_idx]
            
            # Get data for each effort level
            low_effort = []
            high_effort = []
            savings = []
            
            for band in bands:
                low_row = impact_data[(impact_data['start_band'] == band) & (impact_data['effort_bucket'] == 'Low Effort')]
                high_row = impact_data[(impact_data['start_band'] == band) & (impact_data['effort_bucket'] == 'High Effort')]
                
                low_val = low_row[metric].values[0] if not low_row.empty else np.nan
                high_val = high_row[metric].values[0] if not high_row.empty else np.nan
                
                low_effort.append(low_val)
                high_effort.append(high_val)
                savings.append(low_val - high_val if pd.notna(low_val) and pd.notna(high_val) else np.nan)
            
            # Plot bars - calmer colors
            bar_width = 0.35
            bars_low = ax.bar(x_positions - bar_width/2, low_effort, bar_width, 
                             label='Low Effort (Q1)', color='#d98880', alpha=0.85, edgecolor='white', linewidth=1.5)
            bars_high = ax.bar(x_positions + bar_width/2, high_effort, bar_width, 
                              label='High Effort (Q4)', color='#7dcea0', alpha=0.85, edgecolor='white', linewidth=1.5)
            
            # Add value labels on bars
            for bar in bars_low:
                height = bar.get_height()
                if pd.notna(height):
                    ax.annotate(f'{height:.2f}',
                               xy=(bar.get_x() + bar.get_width()/2, height),
                               xytext=(0, 3), textcoords="offset points",
                               ha='center', va='bottom', fontsize=9, fontweight='bold', color='#943126')
            
            for bar in bars_high:
                height = bar.get_height()
                if pd.notna(height):
                    ax.annotate(f'{height:.2f}',
                               xy=(bar.get_x() + bar.get_width()/2, height),
                               xytext=(0, 3), textcoords="offset points",
                               ha='center', va='bottom', fontsize=9, fontweight='bold', color='#1e8449')
            
            # Add savings labels (no arrows, positioned at bottom of chart to avoid overlap)
            max_height = max([v for v in low_effort + high_effort if pd.notna(v)]) if any(pd.notna(v) for v in low_effort + high_effort) else 2
            
            for i, saved in enumerate(savings):
                if pd.notna(saved) and saved > 0:
                    unit = 'EPA' if metric == 'avg_epa' else 'YAC'
                    # Position label below x-axis to avoid overlap with title
                    ax.text(x_positions[i], -0.15 if metric == 'avg_epa' else -0.4, 
                           f'Saved: {saved:.2f} {unit}', 
                           ha='center', va='top', fontsize=9, fontweight='bold',
                           color='#1a5276', 
                           bbox=dict(boxstyle='round,pad=0.2', facecolor='#d5f5e3', edgecolor='#1e8449', alpha=0.9))
            
            # Formatting
            ax.set_xticks(x_positions)
            ax.set_xticklabels(bands, fontsize=10)
            ax.set_xlabel('Starting Distance Band', fontsize=12, fontweight='bold')
            ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')
            ax.set_title(title, fontsize=14, fontweight='bold', pad=15)
            ax.legend(loc='upper right', fontsize=9)
            ax.axhline(0, color='black', linewidth=0.5)
            ax.grid(axis='y', alpha=0.3)
            
            # Set y-limits with padding for labels
            if metric == 'avg_epa':
                ax.set_ylim(bottom=-0.6, top=max_height + 0.5)
            else:
                ax.set_ylim(bottom=-1.0, top=max_height + 0.8)
        
        # Overall title
        fig.suptitle('The Payoff of Erasure: High-Effort Defenders Save Points & Yards', 
                     fontsize=16, fontweight='bold', y=0.98)
        
        plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # Leave room for labels and title
        output_path = os.path.join(self.output_dir, 'V4_Effort_Impact_Chart.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"      -> Saved: {output_path}")
import pandas as pd
import numpy as np

class TableGenerator:
    def __init__(self, summary_path: str):
        self.df = pd.read_csv(summary_path)
        
        # Standardize Role Filtering (Focus on coverage players)
        # We exclude Pass Rushers who occasionally drop into coverage
        self.df = self.df[self.df['player_role'].isin([
            'Defensive Coverage', 'Cornerback', 'Safety', 'Linebacker'])]

    def generate_quadrant_counts(self):
        """
        TABLE 2: Quadrant Counts Table
        Buckets plays into the 4 Matrix Outcomes.
        """
        df = self.df.copy()

        # Define Thresholds
        # OPEN: > 3 yards at throw (A bit tighter than "High Void" to capture more data)
        # CLOSED: < 1.5 yards at arrival
        
        OPEN_THRESH = 3.0
        CLOSED_THRESH = 1.5

        conditions = [
            (df['dist_at_throw'] >= OPEN_THRESH) & (df['dist_at_arrival'] <= CLOSED_THRESH), # Open -> Closed
            (df['dist_at_throw'] < OPEN_THRESH) & (df['dist_at_arrival'] <= CLOSED_THRESH),  # Tight -> Closed
            (df['dist_at_throw'] < OPEN_THRESH) & (df['dist_at_arrival'] > CLOSED_THRESH),  # Tight -> Open
            (df['dist_at_throw'] >= OPEN_THRESH) & (df['dist_at_arrival'] > CLOSED_THRESH)  # Open -> Open
        ]
        
        choices = ['Eraser (The Cleanup)', 'Lockdown (The Blanket)', 'Lost Step (The Beat)', 'Liability (The Void)']
        
        df['quadrant'] = np.select(conditions, choices, default='Neutral/Zone Drift')

        # Aggregation
        summary = df.groupby('quadrant').agg(
            play_count=('play_id', 'count'),
            avg_vis=('vis_score', 'mean'),
            avg_ceoe=('ceoe_score', 'mean')
        ).reset_index()

        summary['avg_vis'] = summary['avg_vis'].round(2)
        summary['avg_ceoe'] = summary['avg_ceoe'].round(3)

        return summary.sort_values('avg_vis', ascending=False)

    def generate_shrunk_leaderboard(self, min_snaps=15, prior_m=20):
        """
        TASK 2: Bayesian Shrinkage with Names.
        """
        # 1. Positional Priors
        pos_stats = self.df.groupby('player_position')['ceoe_score'].mean().to_dict()

        # 2. Add player_name to grouping
        group_cols = ['nfl_id', 'player_position', 'player_role']
        if 'player_name' in self.df.columns:
            group_cols.insert(1, 'player_name')

        player_stats = self.df.groupby(group_cols).agg(
            snaps=('play_id', 'count'),
            raw_ceoe=('ceoe_score', 'mean'),
            avg_vis=('vis_score', 'mean'),
            avg_start=('p_dist_at_throw', 'mean')
        ).reset_index()

        # 3. Shrinkage
        def apply_shrinkage(row):
            prior_mu = pos_stats.get(row['player_position'], 0.0)
            n = row['snaps']
            m = prior_m
            shrunk = ((n * row['raw_ceoe']) + (m * prior_mu)) / (n + m)
            return shrunk

        player_stats['shrunk_ceoe'] = player_stats.apply(apply_shrinkage, axis=1)

        # 4. Filter & Sort
        qualified = player_stats[player_stats['snaps'] >= min_snaps].copy()
        
        qualified['shrunk_ceoe'] = qualified['shrunk_ceoe'].round(3)
        qualified['raw_ceoe'] = qualified['raw_ceoe'].round(3)
        qualified['avg_vis'] = qualified['avg_vis'].round(2)
        qualified['avg_start'] = qualified['avg_start'].round(1)

        top_erasers = qualified.sort_values('shrunk_ceoe', ascending=False).head(10)
        
        return top_erasers

    def generate_damage_control_validation(self):
        """
        TASK 3: Damage Control Validation (YAC & EPA).
        Hypothesis: On COMPLETED passes, higher VIS (better closing) 
        should correlate with LOWER YAC and LOWER EPA (better for defense).
        """
        df = self.df.copy()
        
        # 1. Filter for Completions Only (Where YAC exists)
        completed = df[df['pass_result'] == 'C'].copy()
        
        if completed.empty:
            return "No completions found in dataset."

        # 2. Derive YAC
        # YAC = Total Yards - Air Yards
        completed['yac'] = completed['yards_gained'] - completed['pass_length']
        
        # 3. Bin Start Distance (Context Control)
        # We only care about Medium/High Voids where YAC is a threat.
        bins = [3, 6, 10, 100]
        labels = ['Medium (3-6)', 'High Void (6-10)', 'Deep (10+)']
        completed['start_band'] = pd.cut(completed['p_dist_at_throw'], bins=bins, labels=labels)
        
        # 4. Bin VIS Score (The Independent Variable)
        # Low Effort vs. High Effort closing
        vis_bins = [-np.inf, 0, 3, np.inf]
        vis_labels = ['Negative (Lost Gap)', 'Moderate (0-3)', 'High Erasure (3+)']
        completed['vis_bucket'] = pd.cut(completed['vis_score'], bins=vis_bins, labels=vis_labels)

        # 5. Aggregate
        damage_control = completed.groupby(['start_band', 'vis_bucket'], observed=False).agg(
            count=('play_id', 'count'),
            avg_yac=('yac', 'mean'),
            avg_epa=('expected_points_added', 'mean') # Lower is better for defense
        ).reset_index()

        # 6. Pivot for YAC (The Primary Proof)
        yac_pivot = damage_control.pivot(index='start_band', columns='vis_bucket', values='avg_yac')
        
        # Calculate the "Savings" (Difference between Negative VIS and High Erasure)
        yac_pivot['YAC_Savings'] = yac_pivot['Negative (Lost Gap)'] - yac_pivot['High Erasure (3+)']
        
        return yac_pivot.round(2)

    def generate_epa_savings(self):
        """
        TABLE 5: EPA Savings Table (Quartile Approach).
        Shows how much Expected Points high-effort defenders save vs low-effort defenders.
        Uses within-band quartiles to avoid structural NaNs.
        Focuses on COMPLETED passes where EPA damage occurs.
        """
        df = self.df.copy()
        
        # 1. Filter for Completions Only (where EPA damage occurs)
        completed = df[df['pass_result'] == 'C'].copy()
        
        if completed.empty:
            return "No completions found in dataset."
        
        # 2. Create Start Distance bands
        dist_bins = [0, 3, 6, 10, 100]
        dist_labels = ['Tight (0-3)', 'Medium (3-6)', 'High Void (6-10)', 'Exempt (10+)']
        completed['start_band'] = pd.cut(completed['p_dist_at_throw'], bins=dist_bins, labels=dist_labels)
        
        # 3. Calculate VIS quartiles WITHIN each start band
        # This avoids NaNs by making "effort" relative to what's possible from each start position
        def get_quartile_label(group):
            q25 = group['vis_score'].quantile(0.25)
            q75 = group['vis_score'].quantile(0.75)
            
            conditions = [
                group['vis_score'] <= q25,
                group['vis_score'] >= q75
            ]
            choices = ['Low Effort (Q1)', 'High Effort (Q4)']
            group['effort_bucket'] = np.select(conditions, choices, default='Middle (Q2-Q3)')
            return group
        
        completed = completed.groupby('start_band', group_keys=False, observed=False).apply(get_quartile_label)
        
        # 4. Filter to only Q1 and Q4 for clean comparison
        extremes = completed[completed['effort_bucket'].isin(['Low Effort (Q1)', 'High Effort (Q4)'])]
        
        # 5. Aggregate EPA by Start Band and Effort Bucket
        epa_table = extremes.groupby(['start_band', 'effort_bucket'], observed=False).agg(
            play_count=('play_id', 'count'),
            avg_epa=('expected_points_added', 'mean')
        ).reset_index()
        
        # 6. Pivot for clear comparison
        epa_pivot = epa_table.pivot(index='start_band', columns='effort_bucket', values='avg_epa')
        
        # 7. Calculate EPA Saved (Low Effort EPA - High Effort EPA)
        # Positive = High effort defenders saved points
        if 'Low Effort (Q1)' in epa_pivot.columns and 'High Effort (Q4)' in epa_pivot.columns:
            epa_pivot['EPA_Saved'] = epa_pivot['Low Effort (Q1)'] - epa_pivot['High Effort (Q4)']
        
        # 8. Add play counts for context
        count_pivot = epa_table.pivot(index='start_band', columns='effort_bucket', values='play_count')
        epa_pivot['Plays_Compared'] = count_pivot.sum(axis=1)
        
        # Reorder columns for clarity
        col_order = ['Low Effort (Q1)', 'High Effort (Q4)', 'EPA_Saved', 'Plays_Compared']
        epa_pivot = epa_pivot[[c for c in col_order if c in epa_pivot.columns]]
        
        return epa_pivot.round(3)

    def generate_position_breakdown(self):
        """
        TABLE 6: Position Breakdown - "Who Should Erase?"
        Shows which position groups are best suited for the Eraser role.
        Uses RAW metrics (not shrunk) for position-level comparisons.
        """
        df = self.df.copy()
        
        # 1. Define Eraser criteria (derived from start/end distances, not void_type)
        # Eraser = Started in High Void (>6yds) AND closed to tight (<2yds)
        OPEN_THRESH = 6.0
        CLOSED_THRESH = 2.0
        df['is_eraser_play'] = (df['p_dist_at_throw'] >= OPEN_THRESH) & (df['dist_at_arrival'] <= CLOSED_THRESH)
        
        # 2. Group by player position
        position_stats = df.groupby('player_position').agg(
            play_count=('play_id', 'count'),
            avg_start_dist=('p_dist_at_throw', 'mean'),
            avg_end_dist=('dist_at_arrival', 'mean'),
            avg_vis=('vis_score', 'mean'),
            eraser_plays=('is_eraser_play', 'sum')
        ).reset_index()
        
        # 3. Calculate Eraser Rate (% of plays where they achieved Eraser outcome)
        position_stats['eraser_rate'] = (position_stats['eraser_plays'] / position_stats['play_count'] * 100).round(1)
        
        # 4. Filter for positions with meaningful sample size
        position_stats = position_stats[position_stats['play_count'] >= 50].copy()
        
        # 5. Derive Erasure Archetype based on behavior patterns
        def assign_archetype(row):
            avg_start = row['avg_start_dist']
            avg_vis = row['avg_vis']
            eraser_rate = row['eraser_rate']
            
            # Primary Eraser: Deep starters who close aggressively
            if avg_start >= 8 and avg_vis >= 1.5:
                return "üü¢ Primary Eraser"
            # Secondary Eraser: Medium depth with good closing
            elif avg_start >= 6 and avg_vis >= 1.0:
                return "üîµ Secondary Eraser"
            # Lockdown: Tight coverage specialists (low start = already close)
            elif avg_start < 5 and avg_vis < 0.5:
                return "üü° Lockdown Focus"
            # Situational: High eraser rate despite moderate metrics
            elif eraser_rate >= 5:
                return "üü† Situational Eraser"
            else:
                return "‚ö™ Zone Support"
        
        position_stats['archetype'] = position_stats.apply(assign_archetype, axis=1)
        
        # 6. Format output columns
        position_stats['avg_start_dist'] = position_stats['avg_start_dist'].round(1)
        position_stats['avg_end_dist'] = position_stats['avg_end_dist'].round(1)
        position_stats['avg_vis'] = position_stats['avg_vis'].round(2)
        
        # 7. Select and order columns for output
        # Note: Dropping raw_ceoe as it regresses toward positional means (~0) and confuses readers
        # VIS and archetype provide clearer differentiation
        output_cols = ['player_position', 'play_count', 'avg_start_dist', 'avg_end_dist',
                       'avg_vis', 'eraser_rate', 'archetype']
        
        # Sort by avg_start_dist descending (deep players first = primary erasers)
        return position_stats[output_cols].sort_values('avg_start_dist', ascending=False)

    def generate_void_effect_size(self):
        """
        TABLE: Void Effect Size Analysis
        Shows completion %, EPA, and YAC by S_throw band with effect size 
        (Œî from Tight baseline) to quantify the jump in difficulty.
        """
        df = self.df.copy()
        
        # Define S_throw bands based on dist_at_throw (original separation)
        dist_col = 'p_dist_at_throw' if 'p_dist_at_throw' in df.columns else 'dist_at_throw'
        
        bins = [0, 2, 6, 10, float('inf')]
        labels = ['Tight (0-2 yds)', 'Medium (3-6 yds)', 'High Void (6-10 yds)', 'Deep (10+ yds)']
        df['start_band'] = pd.cut(df[dist_col], bins=bins, labels=labels, include_lowest=True)
        
        # Derive YAC for completions (yards_gained - pass_length)
        df['yac'] = df['yards_gained'] - df['pass_length']
        
        # Calculate metrics by band
        band_stats = df.groupby('start_band', observed=False).agg(
            play_count=('play_id', 'count'),
            completions=('pass_result', lambda x: (x == 'C').sum()),
            avg_epa=('expected_points_added', 'mean')
        ).reset_index()
        
        # Calculate YAC separately (only for completions)
        completed = df[df['pass_result'] == 'C']
        yac_by_band = completed.groupby('start_band', observed=False)['yac'].mean().reset_index()
        yac_by_band.columns = ['start_band', 'avg_yac']
        
        # Merge YAC back
        band_stats = band_stats.merge(yac_by_band, on='start_band', how='left')
        
        # Calculate completion percentage
        band_stats['completion_pct'] = (band_stats['completions'] / band_stats['play_count'] * 100).round(1)
        
        # Calculate effect size (Œî from Tight baseline)
        tight_completion = band_stats.loc[band_stats['start_band'] == 'Tight (0-2 yds)', 'completion_pct'].values
        if len(tight_completion) > 0:
            tight_baseline = tight_completion[0]
            band_stats['delta_from_tight'] = band_stats['completion_pct'] - tight_baseline
            band_stats['delta_from_tight'] = band_stats['delta_from_tight'].apply(
                lambda x: f"+{x:.1f}pp" if x > 0 else ("‚Äî" if x == 0 else f"{x:.1f}pp")
            )
        else:
            band_stats['delta_from_tight'] = "‚Äî"
        
        # Format other columns
        band_stats['avg_epa'] = band_stats['avg_epa'].round(2)
        band_stats['avg_yac'] = band_stats['avg_yac'].round(1)
        band_stats['completion_pct'] = band_stats['completion_pct'].apply(lambda x: f"{x}%")
        
        # Select and rename columns for output
        output = band_stats[['start_band', 'play_count', 'completion_pct', 'delta_from_tight', 'avg_epa', 'avg_yac']]
        output.columns = ['S_throw Band', 'Play Count', 'Completion %', 'Œî from Tight', 'Avg EPA Allowed', 'Avg YAC']
        
        return output
    
if __name__ == "__main__":
    SUMMARY_FILE = "data/processed/eraser_analysis_summary.csv"
    
    gen = TableGenerator(SUMMARY_FILE)
    
    print("\n--- SHRUNK LEADERBOARD (Bayesian m=20) ---")
    print(gen.generate_shrunk_leaderboard().to_string(index=False))
    
    print("\n--- QUADRANT SUMMARY ---")
    print(gen.generate_quadrant_counts().to_string(index=False))

    print("\n--- DAMAGE CONTROL VALIDATION (YAC) ---")
    print(gen.generate_damage_control_validation())

    print("\n--- EPA SAVINGS TABLE ---")
    print(gen.generate_epa_savings())
    
    print("\n--- POSITION BREAKDOWN (Who Should Erase?) ---")
    print(gen.generate_position_breakdown().to_string(index=False))
    
    print("\n--- VOID EFFECT SIZE (Œî from Tight Baseline) ---")
    print(gen.generate_void_effect_size().to_string(index=False))
from story_data_engine import StoryDataEngine
from story_visual_engine import StoryVisualEngine
from animation_engine import AnimationEngine

def main():
    print("=== STARTING VISUALIZATION PIPELINE ===")
    
    # PATHS
    SUMMARY = "data/processed/eraser_analysis_summary.csv"
    ANIMATION = "data/processed/master_animation_data.csv"
    OUTPUT = "static/visuals"

    # Finds the 4 Archetypes automatically based on your strict logic
    print("\n--- STEP 1: CASTING ARCHETYPES ---")
    story = StoryDataEngine(SUMMARY, ANIMATION)
    cast_dict = story.cast_archetypes()
    
    # TODO: Debugging - delete for prod.
    print("Selected Plays:")
    for role, meta in cast_dict.items():
        if meta:
            print(f"   -> {role}: ID {meta['nfl_id']} (VIS: {meta['vis_score']:.1f})")
        else:
            print(f"   -> {role}: [NO CANDIDATE FOUND]")

    # Viz
    viz = StoryVisualEngine(SUMMARY, ANIMATION, OUTPUT)
    viz.plot_eraser_landscape(cast_dict) 
    viz.plot_race_charts(cast_dict)
    viz.plot_coverage_heatmap()
    viz.plot_effort_impact_chart()  # NEW: EPA/YAC Impact Chart

    # Animation
    animator = AnimationEngine(SUMMARY, ANIMATION, OUTPUT)
    
    # We specifically want to animate the "Top Eraser"
    eraser_meta = cast_dict.get('Eraser')
    animator.generate_video(
        game_id=eraser_meta['game_id'], 
        play_id=eraser_meta['play_id'], 
        eraser_id=eraser_meta['nfl_id'], 
        filename="Figure_4_Eraser_Highlight.gif" 
    )
    print("\n=== VISUALIZATION COMPLETE ===")

if __name__ == "__main__":
    main()

