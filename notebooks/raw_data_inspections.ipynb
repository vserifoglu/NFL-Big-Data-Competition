{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fafb19-75ed-444c-8477-92ac4f04af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "520805e3-98fb-4525-a0af-913087329c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/train'\n",
    "SUPP_FILE = '../data/supplementary_data.csv'\n",
    "REPORT_FILE = '../data/dataset_inspection_results.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab84555-e3d6-4ca0-8f77-1080c3681072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Deep Inspection of ../data/train...\n",
      "   Loading Week 1 data for analysis...\n",
      "‚úÖ Inspection Complete. Results saved to ../data/dataset_inspection_results.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def inspect_dataset():\n",
    "    print(f\"üöÄ Starting Deep Inspection of {DATA_DIR}...\")\n",
    "    \n",
    "    with open(REPORT_FILE, 'w') as f:\n",
    "        f.write(\"üèà BIG DATA BOWL 2026: RAW DATASET INSPECTION REPORT\\n\")\n",
    "        f.write(\"====================================================\\n\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # PART 1 & 2: (Kept brief as per previous logic)\n",
    "        # ---------------------------------------------------------\n",
    "        input_files = sorted(glob.glob(os.path.join(DATA_DIR, 'input_*.csv')))\n",
    "        output_files = sorted(glob.glob(os.path.join(DATA_DIR, 'output_*.csv')))\n",
    "        \n",
    "        f.write(f\"Files Found: {len(input_files)} Input pairs, {len(output_files)} Output pairs.\\n\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # PART 3: DEEP DIVE SAMPLE (WEEK 1) - WITH DROPOUT ANALYSIS\n",
    "        # ---------------------------------------------------------\n",
    "        f.write(\"PART 3: WEEK 1 DEEP DIVE & SPARSITY PROOF\\n\")\n",
    "        f.write(\"-----------------------------------------\\n\")\n",
    "        \n",
    "        if input_files and output_files:\n",
    "            # Load Week 1 Pair\n",
    "            w1_input_path = input_files[0]\n",
    "            w1_output_path = output_files[0]\n",
    "            \n",
    "            print(\"   Loading Week 1 data for analysis...\")\n",
    "            df_in = pd.read_csv(w1_input_path, low_memory=False)\n",
    "            df_out = pd.read_csv(w1_output_path, low_memory=False)\n",
    "            \n",
    "            # --- 3.7 ATTRITION & SPARSITY ANALYSIS (NEW) ---\n",
    "            f.write(\"3.7 PLAYER DROPOUT & TEMPORAL SPARSITY ANALYSIS\\n\")\n",
    "            f.write(\"   (Proving that pre_throw frames > post_throw and identifying missing players)\\n\\n\")\n",
    "\n",
    "            # A. Temporal Sparsity (Frame Counts)\n",
    "            # Group by play and count unique frames\n",
    "            in_frames = df_in.groupby(['game_id', 'play_id'])['frame_id'].nunique()\n",
    "            out_frames = df_out.groupby(['game_id', 'play_id'])['frame_id'].nunique()\n",
    "            \n",
    "            # Merge series to compare\n",
    "            frame_comp = pd.concat([in_frames, out_frames], axis=1, keys=['in_frames', 'out_frames']).dropna()\n",
    "            frame_comp['ratio_in_to_out'] = frame_comp['in_frames'] / frame_comp['out_frames']\n",
    "            \n",
    "            f.write(\"   A. Temporal Differences (Frame Counts):\\n\")\n",
    "            f.write(f\"      - Avg Pre-Throw Frames:  {frame_comp['in_frames'].mean():.2f}\\n\")\n",
    "            f.write(f\"      - Avg Post-Throw Frames: {frame_comp['out_frames'].mean():.2f}\\n\")\n",
    "            f.write(f\"      - Conclusion: On average, input files are {frame_comp['ratio_in_to_out'].mean():.2f}x longer than output files.\\n\\n\")\n",
    "\n",
    "            # B. Entity Dropout (Players Disappearing)\n",
    "            f.write(\"   B. Entity Dropout (Players vanishing in Output):\\n\")\n",
    "            \n",
    "            # Get unique NFL IDs per play for Input and Output\n",
    "            # We filter out NaN nfl_ids (usually the football)\n",
    "            in_players = df_in.dropna(subset=['nfl_id']).groupby(['game_id', 'play_id'])['nfl_id'].apply(set)\n",
    "            out_players = df_out.dropna(subset=['nfl_id']).groupby(['game_id', 'play_id'])['nfl_id'].apply(set)\n",
    "            \n",
    "            # Merge to compare sets\n",
    "            dropout_df = pd.concat([in_players, out_players], axis=1, keys=['in_set', 'out_set']).dropna()\n",
    "            \n",
    "            # Calculate dropout\n",
    "            # \"Dropout\" = IDs in Input that are NOT in Output\n",
    "            dropout_df['missing_ids'] = dropout_df.apply(lambda x: x['in_set'] - x['out_set'], axis=1)\n",
    "            dropout_df['missing_count'] = dropout_df['missing_ids'].apply(len)\n",
    "            \n",
    "            total_plays_w1 = len(dropout_df)\n",
    "            plays_with_dropout = len(dropout_df[dropout_df['missing_count'] > 0])\n",
    "            avg_missing = dropout_df['missing_count'].mean()\n",
    "            max_missing = dropout_df['missing_count'].max()\n",
    "            \n",
    "            f.write(f\"      - Total Plays Analyzed: {total_plays_w1}\\n\")\n",
    "            f.write(f\"      - Plays with AT LEAST ONE missing player: {plays_with_dropout} ({plays_with_dropout/total_plays_w1*100:.2f}%)\\n\")\n",
    "            f.write(f\"      - Avg Missing Players per Play: {avg_missing:.2f}\\n\")\n",
    "            f.write(f\"      - Max Missing Players in a single play: {max_missing}\\n\\n\")\n",
    "            \n",
    "            # C. Identify WHO is disappearing (Roles)\n",
    "            if plays_with_dropout > 0:\n",
    "                f.write(\"   C. Profile of Vanishing Players (Sample):\\n\")\n",
    "                # Get a sample play with high dropout\n",
    "                bad_play = dropout_df.sort_values('missing_count', ascending=False).index[0]\n",
    "                missing_ids_list = list(dropout_df.loc[bad_play, 'missing_ids'])\n",
    "                \n",
    "                f.write(f\"      Sample Play (Game {bad_play[0]}, Play {bad_play[1]}):\\n\")\n",
    "                f.write(f\"      Missing {len(missing_ids_list)} players in Output.\\n\")\n",
    "                \n",
    "                # Look up these IDs in the input file to see their roles\n",
    "                missing_details = df_in[\n",
    "                    (df_in.game_id == bad_play[0]) & \n",
    "                    (df_in.play_id == bad_play[1]) & \n",
    "                    (df_in.nfl_id.isin(missing_ids_list))\n",
    "                ][['nfl_id', 'player_name', 'player_role', 'player_position']].drop_duplicates()\n",
    "                \n",
    "                f.write(missing_details.to_string(index=False))\n",
    "                f.write(\"\\n\\n\")\n",
    "                \n",
    "                # Check specific hypothesis: Do specific positions disappear more?\n",
    "                # (Simple check: are they mostly linemen?)\n",
    "                linemen = missing_details['player_position'].isin(['T', 'G', 'C', 'DT', 'NT', 'DE']).mean()\n",
    "                f.write(f\"      Observation: {linemen*100:.1f}% of missing players in this sample are Linemen.\\n\")\n",
    "                f.write(\"      (Hypothesis: Tracking data often drops interior linemen post-throw if they are not near the play.)\\n\\n\")\n",
    "\n",
    "            # Clean up\n",
    "            del df_in, df_out, dropout_df, in_frames, out_frames\n",
    "            gc.collect()\n",
    "\n",
    "        f.write(\"Inspection complete. See 'data_inspection_results.txt' for details.\")\n",
    "\n",
    "    print(f\"‚úÖ Inspection Complete. Results saved to {REPORT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18873db1-d491-4ef1-8922-aff2bf3c11f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Deep Inspection of ../data/train...\n",
      "Index(['game_id', 'season', 'week', 'game_date', 'game_time_eastern',\n",
      "       'home_team_abbr', 'visitor_team_abbr', 'play_id', 'play_description',\n",
      "       'quarter', 'game_clock', 'down', 'yards_to_go', 'possession_team',\n",
      "       'defensive_team', 'yardline_side', 'yardline_number',\n",
      "       'pre_snap_home_score', 'pre_snap_visitor_score',\n",
      "       'play_nullified_by_penalty', 'pass_result', 'pass_length',\n",
      "       'offense_formation', 'receiver_alignment', 'route_of_targeted_receiver',\n",
      "       'play_action', 'dropback_type', 'dropback_distance',\n",
      "       'pass_location_type', 'defenders_in_the_box', 'team_coverage_man_zone',\n",
      "       'team_coverage_type', 'penalty_yards', 'pre_penalty_yards_gained',\n",
      "       'yards_gained', 'expected_points', 'expected_points_added',\n",
      "       'pre_snap_home_team_win_probability',\n",
      "       'pre_snap_visitor_team_win_probability',\n",
      "       'home_team_win_probability_added', 'visitor_team_win_probility_added'],\n",
      "      dtype='object')\n",
      "sdfwefwE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14603/2443698392.py:19: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  supp_df = pd.read_csv(SUPP_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['game_id', 'play_id', 'player_to_predict', 'nfl_id', 'frame_id',\n",
      "       'play_direction', 'absolute_yardline_number', 'player_name',\n",
      "       'player_height', 'player_weight', 'player_birth_date',\n",
      "       'player_position', 'player_side', 'player_role', 'x', 'y', 's', 'a',\n",
      "       'dir', 'o', 'num_frames_output', 'ball_land_x', 'ball_land_y'],\n",
      "      dtype='object') Sdfwe\n",
      "Scanning w01...\n",
      "Scanning w02...\n",
      "Scanning w03...\n",
      "Scanning w04...\n",
      "Scanning w05...\n",
      "Scanning w06...\n",
      "Scanning w07...\n",
      "Scanning w08...\n",
      "Scanning w09...\n",
      "Scanning w10...\n",
      "Scanning w11...\n",
      "Scanning w12...\n",
      "Scanning w13...\n",
      "Scanning w14...\n",
      "Scanning w15...\n",
      "Scanning w16...\n",
      "Scanning w17...\n",
      "Scanning w18...\n",
      "‚úÖ Inspection Complete. Report saved to ../data/dataset_inspection_report_v2.txt\n"
     ]
    }
   ],
   "source": [
    "REPORT_FILE = '../data/dataset_inspection_report_v2.txt'\n",
    "\n",
    "\n",
    "def inspect_dataset():\n",
    "    print(f\"üöÄ Starting Deep Inspection of {DATA_DIR}...\")\n",
    "    \n",
    "    with open(REPORT_FILE, 'w') as f:\n",
    "        f.write(\"üèà BIG DATA BOWL 2026: RAW DATASET INSPECTION REPORT\\n\")\n",
    "        f.write(f\"Generated on: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(\"====================================================\\n\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # PART 1: SUPPLEMENTARY DATA INSPECTION\n",
    "        # ---------------------------------------------------------\n",
    "        f.write(\"PART 1: SUPPLEMENTARY DATA (METADATA)\\n\")\n",
    "        f.write(\"-------------------------------------\\n\")\n",
    "        \n",
    "        if os.path.exists(SUPP_FILE):\n",
    "            supp_df = pd.read_csv(SUPP_FILE)\n",
    "            f.write(f\"File: {SUPP_FILE}\\n\")\n",
    "            f.write(f\"Shape: {supp_df.shape}\\n\")\n",
    "            f.write(f\"Columns: {list(supp_df.columns)}\\n\\n\")\n",
    "            \n",
    "            # Critical Field Analysis\n",
    "            f.write(\">>> Coverage Types Distribution:\\n\")\n",
    "            f.write(supp_df['team_coverage_man_zone'].value_counts(dropna=False).to_string())\n",
    "            f.write(\"\\n\\n\")\n",
    "            \n",
    "            f.write(\">>> Pass Results:\\n\")\n",
    "            f.write(supp_df['pass_result'].value_counts(dropna=False).to_string())\n",
    "            f.write(\"\\n\\n\")\n",
    "            \n",
    "            f.write(\">>> Null Values in Critical Columns:\\n\")\n",
    "            critical_cols = ['game_id', 'play_id', 'team_coverage_type', 'pass_length']\n",
    "            print(supp_df.columns)\n",
    "            f.write(supp_df[critical_cols].isnull().sum().to_string())\n",
    "            print(\"sdfwefwE\")\n",
    "            f.write(\"\\n\\n\")\n",
    "        else:\n",
    "            f.write(f\"‚ùå ERROR: Supplementary file not found at {SUPP_FILE}\\n\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # PART 2: TRACKING DATA FILE TOPOLOGY\n",
    "        # ---------------------------------------------------------\n",
    "        f.write(\"PART 2: TRACKING DATA TOPOLOGY\\n\")\n",
    "        f.write(\"------------------------------\\n\")\n",
    "        input_files = sorted(glob.glob(os.path.join(DATA_DIR, 'input_*.csv')))\n",
    "        output_files = sorted(glob.glob(os.path.join(DATA_DIR, 'output_*.csv')))\n",
    "        \n",
    "        f.write(f\"Total Input Files (Pre-Throw): {len(input_files)}\\n\")\n",
    "        f.write(f\"Total Output Files (Post-Throw): {len(output_files)}\\n\\n\")\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # PART 3: DEEP DIVE SAMPLE (WEEK 1)\n",
    "        # ---------------------------------------------------------\n",
    "        f.write(\"PART 3: DEEP SAMPLE ANALYSIS (WEEK 1)\\n\")\n",
    "        f.write(\"-------------------------------------\\n\")\n",
    "        \n",
    "        if input_files and output_files:\n",
    "            # Load Week 1 Pair\n",
    "            w1_input_path = input_files[0]\n",
    "            w1_output_path = output_files[0]\n",
    "            \n",
    "            f.write(f\"Loading Sample Pair:\\n  - {os.path.basename(w1_input_path)}\\n  - {os.path.basename(w1_output_path)}\\n\\n\")\n",
    "            \n",
    "            df_in = pd.read_csv(w1_input_path, low_memory=False)\n",
    "            df_out = pd.read_csv(w1_output_path, low_memory=False)\n",
    "            \n",
    "            # 3.1 Column Integrity\n",
    "            f.write(\"3.1 Column Consistency Check:\\n\")\n",
    "            in_cols = set(df_in.columns)\n",
    "            out_cols = set(df_out.columns)\n",
    "            \n",
    "            if in_cols == out_cols:\n",
    "                f.write(\"‚úÖ Input and Output files have identical schemas.\\n\")\n",
    "                f.write(f\"Columns: {list(df_in.columns)}\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"‚ö†Ô∏è Schema Mismatch detected!\\n\")\n",
    "                f.write(f\"Only in Input: {in_cols - out_cols}\\n\")\n",
    "                f.write(f\"Only in Output: {out_cols - in_cols}\\n\\n\")\n",
    "\n",
    "            # 3.2 Basic Stats\n",
    "            f.write(\"3.2 Sample Statistics (Week 1):\\n\")\n",
    "            f.write(f\"Input Rows (Pre-Throw): {len(df_in):,}\\n\")\n",
    "            f.write(f\"Output Rows (Post-Throw): {len(df_out):,}\\n\")\n",
    "            \n",
    "            # 3.3 Stitching Logic Check\n",
    "            f.write(\"\\n3.3 Stitching Logic Inspection:\\n\")\n",
    "            sample_play = df_in[['game_id', 'play_id']].iloc[0]\n",
    "            g_id, p_id = sample_play['game_id'], sample_play['play_id']\n",
    "            \n",
    "            sample_in = df_in[(df_in.game_id == g_id) & (df_in.play_id == p_id)]\n",
    "            sample_out = df_out[(df_out.game_id == g_id) & (df_out.play_id == p_id)]\n",
    "            \n",
    "            max_frame_in = sample_in['frame_id'].max()\n",
    "            min_frame_out = sample_out['frame_id'].min()\n",
    "            \n",
    "            f.write(f\"Sample Play ({g_id} - {p_id}):\\n\")\n",
    "            f.write(f\"  - Max Input Frame: {max_frame_in}\\n\")\n",
    "            f.write(f\"  - Min Output Frame: {min_frame_out} (Should be 1)\\n\")\n",
    "            f.write(f\"  - Gap Analysis: Input ends at {max_frame_in}, Output starts at {min_frame_out}. Offset required.\\n\\n\")\n",
    "\n",
    "            # 3.4 Player Roles\n",
    "            f.write(\"3.4 Player Roles in Tracking Data:\\n\")\n",
    "            roles = df_in['player_role'].dropna().unique()\n",
    "            f.write(f\"{', '.join(roles)}\\n\\n\")\n",
    "            \n",
    "            # 3.5 Coordinate Range (Normalization Check)\n",
    "            f.write(\"3.5 Coordinate Bounds (Raw Data):\\n\")\n",
    "            f.write(f\"  - X Range: {df_in['x'].min()} to {df_in['x'].max()}\\n\")\n",
    "            f.write(f\"  - Y Range: {df_in['y'].min()} to {df_in['y'].max()}\\n\")\n",
    "            f.write(\"  *Note: If X goes > 100, standard NFL coords (0-120) are likely used.*\\n\\n\")\n",
    "\n",
    "            # 3.6 Ball Landing Spot availability\n",
    "            f.write(\"3.6 Ball Landing Spot Availability:\\n\")\n",
    "            print(df_in.columns, \"Sdfwe\")\n",
    "            null_ball = df_in['ball_land_x'].isnull().mean() * 100\n",
    "            f.write(f\"  - Percentage of rows with Missing 'ball_land_x': {null_ball:.2f}%\\n\")\n",
    "            if null_ball > 0:\n",
    "                f.write(\"  - (This is expected if ball_land is only populated on specific frames or plays)\\n\\n\")\n",
    "\n",
    "            # Clean up memory\n",
    "            del df_in, df_out, sample_in, sample_out\n",
    "            gc.collect()\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # PART 4: AGGREGATE SCAN (ALL FILES)\n",
    "        # ---------------------------------------------------------\n",
    "        f.write(\"PART 4: AGGREGATE SCAN (ALL WEEKS)\\n\")\n",
    "        f.write(\"----------------------------------\\n\")\n",
    "        \n",
    "        total_rows = 0\n",
    "        total_games = set()\n",
    "        total_plays = set() # Store tuples (game_id, play_id)\n",
    "        \n",
    "        # Iterate through pairs to save memory\n",
    "        for i_path, o_path in zip(input_files, output_files):\n",
    "            week_num = i_path.split('_')[-1].split('.')[0] # e.g., w01\n",
    "            print(f\"Scanning {week_num}...\")\n",
    "            \n",
    "            # Just read minimal columns to get counts\n",
    "            cols_to_load = ['game_id', 'play_id']\n",
    "            \n",
    "            # Read chunks to avoid memory spike\n",
    "            df_i = pd.read_csv(i_path, usecols=cols_to_load, low_memory=False)\n",
    "            df_o = pd.read_csv(o_path, usecols=cols_to_load, low_memory=False)\n",
    "            \n",
    "            current_rows = len(df_i) + len(df_o)\n",
    "            total_rows += current_rows\n",
    "            \n",
    "            # Update Unique Sets\n",
    "            week_games = set(df_i['game_id'].unique())\n",
    "            week_plays = set(zip(df_i['game_id'], df_i['play_id']))\n",
    "            \n",
    "            total_games.update(week_games)\n",
    "            total_plays.update(week_plays)\n",
    "            \n",
    "            f.write(f\"  - {week_num}: {current_rows:,} rows | {len(week_games)} games | {len(week_plays)} plays\\n\")\n",
    "            \n",
    "            del df_i, df_o\n",
    "            gc.collect()\n",
    "\n",
    "        f.write(f\"\\nTOTAL DATASET STATS:\\n\")\n",
    "        f.write(f\"  - Total Rows: {total_rows:,}\\n\")\n",
    "        f.write(f\"  - Unique Games: {len(total_games)}\\n\")\n",
    "        f.write(f\"  - Unique Plays: {len(total_plays)}\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Inspection Complete. Report saved to {REPORT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        inspect_dataset()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to run inspection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1265b499-a0a6-49e2-b29c-8edab9d50175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è Analyzing Player Dropout based on Ball Landing Spot...\n",
      "Results saved to ../data/dataset_dropout_logic_inspection.txt\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = '../data/dataset_dropout_logic_inspection.txt'\n",
    "\n",
    "def inspect_dropout_logic():\n",
    "    print(\"üïµÔ∏è Analyzing Player Dropout based on Ball Landing Spot...\")\n",
    "    \n",
    "    # Load Week 1 data\n",
    "    input_files = sorted(glob.glob(os.path.join(DATA_DIR, 'input_*.csv')))\n",
    "    output_files = sorted(glob.glob(os.path.join(DATA_DIR, 'output_*.csv')))\n",
    "    \n",
    "    # We use low_memory=False to ensure columns don't get mixed types\n",
    "    df_in = pd.read_csv(input_files[0], low_memory=False)\n",
    "    df_out = pd.read_csv(output_files[0], low_memory=False)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Analyze a sample of 100 plays\n",
    "    unique_plays = df_in[['game_id', 'play_id']].drop_duplicates().head(100).values\n",
    "    \n",
    "    for g_id, p_id in unique_plays:\n",
    "        # 1. Get the \"Before\" Snapshot: The LAST frame of the Input file\n",
    "        # This is the split-second BEFORE the throw (or the instant of the throw in input data).\n",
    "        play_in = df_in[(df_in.game_id == g_id) & (df_in.play_id == p_id)]\n",
    "        if play_in.empty: continue\n",
    "            \n",
    "        last_frame_id = play_in['frame_id'].max()\n",
    "        snapshot = play_in[play_in['frame_id'] == last_frame_id].copy()\n",
    "        \n",
    "        # 2. Get the Ball Landing Spot (From the Input columns)\n",
    "        # Note: ball_land_x/y only exist in the Input file.\n",
    "        if 'ball_land_x' not in snapshot.columns:\n",
    "            continue\n",
    "            \n",
    "        ball_land_x = snapshot['ball_land_x'].iloc[0]\n",
    "        ball_land_y = snapshot['ball_land_y'].iloc[0]\n",
    "        \n",
    "        if pd.isna(ball_land_x) or pd.isna(ball_land_y):\n",
    "            continue\n",
    "\n",
    "        # 3. Who Survived? (Check Output Frame 1)\n",
    "        # We verify who made it to the \"Moment of the Throw\" data.\n",
    "        play_out = df_out[(df_out.game_id == g_id) & (df_out.play_id == p_id)]\n",
    "        survivor_ids = set(play_out['nfl_id'].unique())\n",
    "        \n",
    "        # 4. Calculate Distance to the LANDING SPOT\n",
    "        # We calculate this on the Snapshot because the \"Ghosts\" are still present here.\n",
    "        snapshot['dist_to_landing'] = np.sqrt(\n",
    "            (snapshot['x'] - ball_land_x)**2 + \n",
    "            (snapshot['y'] - ball_land_y)**2\n",
    "        )\n",
    "        \n",
    "        # 5. Tag Survivors vs Ghosts\n",
    "        snapshot = snapshot.dropna(subset=['nfl_id'])\n",
    "        snapshot['status'] = snapshot['nfl_id'].apply(lambda x: 'Survivor' if x in survivor_ids else 'Ghost')\n",
    "        \n",
    "        results.append(snapshot[['game_id', 'play_id', 'nfl_id', 'player_role', 'dist_to_landing', 'status']])\n",
    "\n",
    "    # --- AGGREGATE RESULTS ---\n",
    "    if not results:\n",
    "        print(\"No valid plays found.\")\n",
    "        return\n",
    "\n",
    "    all_data = pd.concat(results)\n",
    "    \n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write(\"THE IRRELEVANCE HYPOTHESIS TEST\\n\")\n",
    "        f.write(\"===============================\\n\")\n",
    "        f.write(\"Hypothesis: Players are dropped in the Output file because they are far from the landing spot.\\n\\n\")\n",
    "        \n",
    "        # 1. Compare Distances\n",
    "        stats = all_data.groupby('status')['dist_to_landing'].describe()\n",
    "        f.write(\"1. Distance to Landing Spot (Stats):\\n\")\n",
    "        f.write(stats.to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # 2. The Danger Check\n",
    "        # Are there Ghosts within 15 yards of where the ball landed?\n",
    "        THRESHOLD = 15.0\n",
    "        danger_ghosts = all_data[(all_data['status'] == 'Ghost') & (all_data['dist_to_landing'] < THRESHOLD)]\n",
    "        \n",
    "        f.write(f\"2. Danger Ghosts (< {THRESHOLD} yards from landing):\\n\")\n",
    "        f.write(f\"   Count: {len(danger_ghosts)}\\n\")\n",
    "        \n",
    "        if len(danger_ghosts) > 0:\n",
    "            f.write(\"   ‚ö†Ô∏è WARNING: Some players close to the catch point are disappearing!\\n\")\n",
    "            f.write(\"   Sample of these missing players:\\n\")\n",
    "            f.write(danger_ghosts[['game_id', 'play_id', 'player_role', 'dist_to_landing']].head(15).to_string())\n",
    "        else:\n",
    "            f.write(\"   ‚úÖ SUCCESS: All missing players were > 15 yards from the landing spot.\\n\")\n",
    "            f.write(\"   (This confirms the dataset filters out irrelevant players.)\\n\")\n",
    "            \n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        # 3. Role Analysis\n",
    "        if 'player_role' in all_data.columns:\n",
    "            targets = all_data[all_data['player_role'] == 'Targeted Receiver']\n",
    "            missing_targets = targets[targets['status'] == 'Ghost']\n",
    "            \n",
    "            f.write(f\"3. Missing Targeted Receivers: {len(missing_targets)}\\n\")\n",
    "            if len(missing_targets) > 0:\n",
    "                f.write(\"   ‚ùå CRITICAL: Targeted Receivers are dropping out!\\n\")\n",
    "            else:\n",
    "                f.write(\"   ‚úÖ PASS: Targeted Receiver always survives.\\n\")\n",
    "\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_dropout_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8997aa61-e1d0-4753-8291-7a837a614b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Inspecting Physics Variables (s, a, dir, o)...\n",
      "Inspection complete. Check ../data/physics_vars_inspection.txt\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = '../data/physics_vars_inspection.txt'\n",
    "\n",
    "def inspect_physics_vars():\n",
    "    print(\"üî¨ Inspecting Physics Variables (s, a, dir, o)...\")\n",
    "    \n",
    "    input_files = sorted(glob.glob(os.path.join(DATA_DIR, 'input_*.csv')))\n",
    "    output_files = sorted(glob.glob(os.path.join(DATA_DIR, 'output_*.csv')))\n",
    "    \n",
    "    # Load Week 1 pair\n",
    "    df_in = pd.read_csv(input_files[0], nrows=1000) # Just need headers and a few rows\n",
    "    df_out = pd.read_csv(output_files[0], nrows=1000)\n",
    "    \n",
    "    PHYSICS_COLS = ['s', 'a', 'dir', 'o']\n",
    "    \n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write(\"PHYSICS VARIABLES AVAILABILITY REPORT\\n\")\n",
    "        f.write(\"=======================================\\n\\n\")\n",
    "        \n",
    "        # 1. Check Column Existence\n",
    "        f.write(\"1. Column Existence Check:\\n\")\n",
    "        \n",
    "        in_missing = [c for c in PHYSICS_COLS if c not in df_in.columns]\n",
    "        out_missing = [c for c in PHYSICS_COLS if c not in df_out.columns]\n",
    "        \n",
    "        f.write(f\"   Input Files Missing: {in_missing if in_missing else 'None (All present)'}\\n\")\n",
    "        f.write(f\"   Output Files Missing: {out_missing if out_missing else 'None (All present)'}\\n\\n\")\n",
    "        \n",
    "        # 2. Check for Nulls (in case columns exist but are empty)\n",
    "        f.write(\"2. Data Content Check (Are they full of NaNs?):\\n\")\n",
    "        \n",
    "        f.write(\"   [INPUT FILES]\\n\")\n",
    "        for col in PHYSICS_COLS:\n",
    "            if col in df_in.columns:\n",
    "                null_pct = df_in[col].isnull().mean() * 100\n",
    "                f.write(f\"     - {col}: {null_pct:.1f}% Nulls\\n\")\n",
    "            else:\n",
    "                f.write(f\"     - {col}: COLUMN MISSING\\n\")\n",
    "                \n",
    "        f.write(\"\\n   [OUTPUT FILES]\\n\")\n",
    "        for col in PHYSICS_COLS:\n",
    "            if col in df_out.columns:\n",
    "                null_pct = df_out[col].isnull().mean() * 100\n",
    "                f.write(f\"     - {col}: {null_pct:.1f}% Nulls\\n\")\n",
    "            else:\n",
    "                f.write(f\"     - {col}: COLUMN MISSING\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # 3. Implication Analysis\n",
    "        if out_missing or any(df_out[c].isnull().all() for c in PHYSICS_COLS if c in df_out.columns):\n",
    "            f.write(\"3. CRITICAL IMPLICATIONS:\\n\")\n",
    "            f.write(\"   The Output files lack motion vectors.\\n\")\n",
    "            f.write(\"   IMPACT 1 (Animation): You cannot use 'dir' arrows in visualization.\\n\")\n",
    "            f.write(\"   IMPACT 2 (Normalization): 'data_preprocessor.py' tries to flip 'dir'/'o' for left-moving plays.\\n\")\n",
    "            f.write(\"             If these columns are missing, that code block will crash.\\n\")\n",
    "            f.write(\"   IMPACT 3 (Physics): You must DERIVE speed/direction from x,y changes if needed.\\n\")\n",
    "        else:\n",
    "            f.write(\"3. Status: Green. All physics variables are available.\\n\")\n",
    "\n",
    "    print(f\"Inspection complete. Check {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_physics_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d82a3a-14be-4a73-b078-5d0abcc332d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
