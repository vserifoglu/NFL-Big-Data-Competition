import pandera.pandas as pa
from pandera.typing import Series


class RawSuppSchema(pa.DataFrameModel):
    """
    Validates 'supplementary_data.csv'.
    Contains Game Context, Play Types, and Outcomes.
    """
    # --- Identifiers ---
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    
    # --- Plotting & Context ---
    week: Series[int] = pa.Field(coerce=True)
    home_team_abbr: Series[str]
    visitor_team_abbr: Series[str]
    play_description: Series[str] = pa.Field(nullable=True)
    
    # --- Game State ---
    down: Series[int] = pa.Field(coerce=True, ge=1, le=4)
    yards_to_go: Series[int] = pa.Field(coerce=True)
    possession_team: Series[str]
    defensive_team: Series[str]
    yardline_number: Series[int] = pa.Field(ge=0, le=50) 
    pre_snap_home_team_win_probability: Series[float] = pa.Field(nullable=True)
    pre_snap_visitor_team_win_probability: Series[float] = pa.Field(nullable=True)

    # --- Logic Filters (Critical) ---
    play_nullified_by_penalty: Series[str] = pa.Field(nullable=True)
    dropback_type: Series[str] = pa.Field(nullable=True) 
    play_action: Series[bool] = pa.Field(coerce=True, nullable=True)
    
    # --- Defensive Scheme ---
    team_coverage_man_zone: Series[str] = pa.Field(nullable=True)
    team_coverage_type: Series[str] = pa.Field(nullable=True) 
    
    # --- Results & Geometry ---
    pass_result: Series[str] = pa.Field(nullable=True) 
    pass_length: Series[int] = pa.Field(nullable=True)
    pass_location_type: Series[str] = pa.Field(nullable=True) 
    yards_gained: Series[int] = pa.Field(nullable=True)
    expected_points_added: Series[float] = pa.Field(nullable=True)
    route_of_targeted_receiver: Series[str] = pa.Field(nullable=True)

    class Config:
        strict = 'filter' 
        

class RawTrackingSchema(pa.DataFrameModel):
    """
    Validates 'input_wXX.csv' files (Pre-Throw / Full Play).
    """
    # --- Identifiers ---
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    frame_id: Series[int] = pa.Field(coerce=True, ge=1)
    nfl_id: Series[float] = pa.Field(coerce=True, nullable=True) # Nullable for Ball

    # --- Normalization Anchors ---
    play_direction: Series[str] 
    absolute_yardline_number: Series[int] = pa.Field(ge=0, le=120)
    
    # --- Player Attributes ---
    player_name: Series[str] = pa.Field(nullable=True)
    
    player_position: Series[str] = pa.Field(nullable=True)
    player_side: Series[str] = pa.Field(nullable=True) 
    player_role: Series[str] = pa.Field(nullable=True)
    
    # --- Physics Vectors ---
    x: Series[float] = pa.Field(ge=0, nullable=True)
    y: Series[float] = pa.Field(ge=0, nullable=True)
    s: Series[float] = pa.Field(ge=0, nullable=True) 
    a: Series[float] = pa.Field(ge=0, nullable=True) 
    o: Series[float] = pa.Field(ge=0, nullable=True) 
    dir: Series[float] = pa.Field(ge=0, nullable=True)
    
    # --- Answer Key ---
    ball_land_x: Series[float] = pa.Field(nullable=True)
    ball_land_y: Series[float] = pa.Field(nullable=True)

    class Config:
        strict = 'filter' 
        

class OutputTrackingSchema(pa.DataFrameModel):
    """
    Validates 'output_wXX.csv' files (Post-Throw Frames).
    Does NOT require speed/accel/dir.
    """
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    nfl_id: Series[int] = pa.Field(coerce=True, nullable=True)
    frame_id: Series[int] = pa.Field(coerce=True)
    
    x: Series[float] = pa.Field(ge=0, nullable=True)
    y: Series[float] = pa.Field(ge=0, nullable=True)

    class Config:
        strict = 'filter'


class PreprocessedSchema(RawTrackingSchema, RawSuppSchema):
    """
    Validates the output of 'preprocessing.py'.
    Inherits RawTracking + RawSupp.
    Adds calculated fields from the ETL process.
    """
    
    phase: Series[str] = pa.Field(isin=["pre_throw", "post_throw"])
    los_x: Series[float] = pa.Field(nullable=True) # Calculated Line of Scrimmage
    
    event: Series[str] = pa.Field(nullable=True) 

    class Config:
        strict = 'filter'


class PhysicsSchema(PreprocessedSchema):
    """
    Validates the output of 'physics_engine.py'.
    Inherits Preprocessed data and adds the Savitzky-Golay derived metrics.
    """
    # [NEW] Derived Physics
    s_derived: Series[float] = pa.Field(nullable=True, coerce=True)
    a_derived: Series[float] = pa.Field(nullable=True, coerce=True)

    # 0-360 degrees
    dir_derived: Series[float] = pa.Field(nullable=True, coerce=True) 
    
    class Config:
        strict = 'filter'


class FeatureEngineeredSchema(PhysicsSchema):
    """
    Validates the output of 'features.py' (Landmark Calculation).
    Inherits PreprocessedSchema.
    Adds Zone Assignments and Target Coordinates.
    """

    # Angle from Player to Ball
    dir_ideal: Series[float] = pa.Field(nullable=True, ge=0, le=360) 

    # Deviation (Efficiency Error)
    angle_diff: Series[float] = pa.Field(nullable=True, ge=0, le=180) 

    class Config:
        strict = 'filter'


class VoidDetectiontSchema(pa.DataFrameModel):
    """
    Validates the output of (Void Detection).
    """
    
    # Grouping Keys (REQUIRED for merging)
    game_id: Series[int] = pa.Field(coerce=True)
    play_id: Series[int] = pa.Field(coerce=True)
    nfl_id: Series[float] = pa.Field(coerce=True)
    
    # Calculation Results
    reaction_time_frames: Series[float] = pa.Field(nullable=True) 
    is_reaction_void: Series[bool]      
    avg_pursuit_error: Series[float] = pa.Field(nullable=True) 
    
    class Config:
        strict = 'filter'
import os
import glob
import re
import pandas as pd
from typing import Generator, Tuple
from schema import RawTrackingSchema, OutputTrackingSchema, RawSuppSchema


class DataLoader:
    def __init__(self, data_dir: str, supp_file: str):
        """
        Scans the directory for files but DOES NOT load them yet.
        """
        self.data_dir = data_dir
        self.supp_file = supp_file
        
        # 1. Find all files
        self.input_files = sorted(glob.glob(os.path.join(self.data_dir, 'input_*.csv')))
        self.output_files = glob.glob(os.path.join(self.data_dir, 'output_*.csv'))
        
        self.output_map = {}
        for f in self.output_files:
            match = re.search(r'w(\d{2})', f)
            if not match:
                continue
            self.output_map[match.group(1)] = f

    def load_supplementary(self) -> pd.DataFrame:
        """
        Loads the single Supplementary file.
        """
        if not os.path.exists(self.supp_file):
            raise FileNotFoundError(f"Missing Supp File: {self.supp_file}")
            
        df = pd.read_csv(self.supp_file, low_memory=False)
            
        # VALIDATE (Strict Filter)
        return RawSuppSchema.validate(df)

    def stream_weeks(self) -> Generator[Tuple[str, pd.DataFrame, pd.DataFrame], None, None]:
        """
        The Lazy Loader.
        Yields: (week_num, input_df, output_df)
        
        Validation happens JUST-IN-TIME here.
        """

        count = 0
        for input_path in self.input_files:
            # if count > 0: break
            # Extract Week Number
            match = re.search(r'w(\d{2})', input_path)
            
            if not match: continue
            week_num = match.group(1)
            
            output_path = self.output_map.get(week_num)

            print(f"Streaming Week {week_num}...")
            
            # Load from Disk
            input_raw = pd.read_csv(input_path, low_memory=False)
            output_raw = pd.read_csv(output_path, low_memory=False)
            
            input_raw['nfl_id'] = pd.to_numeric(input_raw['nfl_id'], errors='coerce')
            output_raw['nfl_id'] = pd.to_numeric(output_raw['nfl_id'], errors='coerce')

            # VALIDATE
            input_valid = RawTrackingSchema.validate(input_raw)
            output_valid = OutputTrackingSchema.validate(output_raw)
            
            # count += 1
            # Yield the clean, validated data to the Orchestrator
            yield week_num, input_valid, output_valid
import pandas as pd
import numpy as np
import gc
from typing import Generator, Tuple, List
from schema import PreprocessedSchema

class DataPreProcessor:
    def __init__(self):
        self.output_schema = PreprocessedSchema
        self.keep_cols = list(self.output_schema.to_schema().columns.keys())

    def filter_context(self, supp_df):
        """
        filter the supplementary dataframe.
        """

        # filter out low win probability to reduce outlier & unusual games.
        supp_df['possession_win_prob'] = np.where(
            supp_df['possession_team'] == supp_df['home_team_abbr'],
            supp_df['pre_snap_home_team_win_probability'],
            supp_df['pre_snap_visitor_team_win_probability'],
        )
        
        valid_mask = (
            (supp_df['team_coverage_man_zone'].astype(str).str.contains('Zone', case=False, na=False)) &
            (supp_df['pass_result'].isin(['C', 'I', 'IN'])) &
            (supp_df['team_coverage_type'] != 'COVER_6_ZONE') &
            (~supp_df['dropback_type'].isin(['SCRAMBLE', 'SCRAMBLE_ROLLOUT_LEFT', 'SCRAMBLE_ROLLOUT_RIGHT', 'QB_DRAW'])) &
            (supp_df['play_nullified_by_penalty'] != 'Y')
        )


        screen_shovel_mask = (
            # Identify Screen Routes (Text Search)
            supp_df['route_of_targeted_receiver'].astype(str).str.upper().str.contains('SCREEN', na=False) | 
            
            # Identify Behind-the-Line Passes.
            # pass_length < 0 implies the ball was caught behind the LOS (Screens, Swings, Shovels).
            supp_df['pass_length'] <= 0 | 
            
            # 3. Identify 'Flat' routes that are also very short
            # Some 'FLAT' routes are effectively check-downs/screens.
            (
                (supp_df['route_of_targeted_receiver'].astype(str).str.upper() == 'FLAT') & 
                (supp_df['pass_length'] < 3)
            )
        
        )
        
        base_situation_mask = (           
            # Standard Downs
            (supp_df['down'].isin([1, 2])) &
            
            # Competitive Game (Win Prob between 20% and 80%)
            (supp_df['possession_win_prob'].between(0.20, 0.80)) & 

            # Removes 1st & 20 (penalties) and 2nd & Long (sacks)
            (supp_df['yards_to_go'] <= 10)
        )
        
        # combine all masks.
        final_valid_mask = (
            valid_mask &
            ~ screen_shovel_mask &
            base_situation_mask
        )

        return supp_df[valid_mask].copy()

    def _stitch_tracking_data(self, input_df, output_df, valid_keys):
        """
        Pure Logic. Merges Pre-Throw and Post-Throw data.
        """
        # Filter Input
        input_df['key_tuple'] = list(zip(input_df.game_id, input_df.play_id))
        input_df = input_df[input_df['key_tuple'].isin(valid_keys)].drop(columns=['key_tuple'])
        input_df['phase'] = 'pre_throw'
        
        if output_df.empty: return input_df

        # Filter Output
        output_df['key_tuple'] = list(zip(output_df.game_id, output_df.play_id))
        output_df = output_df[output_df['key_tuple'].isin(valid_keys)].drop(columns=['key_tuple'])
        
        # Logic: Tag first frame of Output as pass_forward
        output_df['event'] = None
        output_df.loc[output_df['frame_id'] == 1, 'event'] = 'pass_forward'
        
        # Metadata Propagation
        meta_cols = ['game_id', 'play_id', 'nfl_id', 'player_name', 'jersey_number', 'player_position', 
                     'player_role', 'player_side', 'play_direction', 'absolute_yardline_number', 
                     'ball_land_x', 'ball_land_y']
        
        avail_cols = [c for c in meta_cols if c in input_df.columns]
        player_meta = input_df[avail_cols].drop_duplicates(subset=['game_id', 'play_id', 'nfl_id'])
        
        # Frame Offset
        play_offsets = input_df.groupby(['game_id', 'play_id'])['frame_id'].max().reset_index()
        play_offsets.columns = ['game_id', 'play_id', 'offset']
        
        output_df = output_df.merge(player_meta, on=['game_id', 'play_id', 'nfl_id'], how='left')
        output_df = output_df.merge(play_offsets, on=['game_id', 'play_id'], how='left')
        
        output_df['frame_id'] = output_df['frame_id'] + output_df['offset'].fillna(0)
        output_df['phase'] = 'post_throw'
        
        df = pd.concat([input_df, output_df.drop(columns=['offset'])], ignore_index=True)

        return df

    def _normalize_coordinates(self, df):
        if 'play_direction' not in df.columns: return df
        mask = df['play_direction'].str.lower() == 'left'
        
        for col in ['x', 'ball_land_x']:
            if col in df.columns: df.loc[mask, col] = 120 - df.loc[mask, col]

        for col in ['y', 'ball_land_y']:
            if col in df.columns: df.loc[mask, col] = 53.3 - df.loc[mask, col]

        return df

    def _clean_and_deduplicate(self, df):
        df['phase_rank'] = df['phase'].apply(lambda x: 1 if x == 'pre_throw' else 2)        
        
        df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id', 'phase_rank'])        
        
        df = df.drop_duplicates(subset=['game_id', 'play_id', 'nfl_id', 'frame_id'], keep='last')
        
        return df.drop(columns=['phase_rank'])

    def process_single_week(self, week_num, input_df, output_df, context_df):
        """
        Internal logic for a single week.
        """
        valid_keys = set(zip(context_df.game_id, context_df.play_id))

        # 1. Stitch
        week_df = self._stitch_tracking_data(input_df, output_df, valid_keys)

        # 2. Merge Context
        week_df = week_df.merge(context_df, on=['game_id', 'play_id'], how='inner')

        # 3. Normalize
        week_df = self._normalize_coordinates(week_df)

        # 4. Features
        week_df['los_x'] = week_df['ball_land_x'] - week_df['pass_length']
        week_df['week'] = int(week_num)

        # 5. Clean
        week_df = self._clean_and_deduplicate(week_df)

        # 6. Validate Output Schema
        return self.output_schema.validate(week_df)

    def run(self, data_stream: Generator[Tuple[str, pd.DataFrame, pd.DataFrame], None, None], 
            raw_context_df: pd.DataFrame) -> pd.DataFrame:
        """
        MAIN ENTRY POINT.
        1. Filters Context.
        2. Consumes the data stream.
        3. Processes week-by-week.
        4. Returns final concatenated dataframe.
        """
        clean_context = self.filter_context(raw_context_df)
        
        processed_chunks: List[pd.DataFrame] = []
        
        for week_num, input_df, output_df in data_stream:
            # Open Field (Between the 20s, avoiding backed-up safety or redzone)
            # (supp_df['absolute_yardline_number'].between(20, 90))
            # input_df = input_df[input_df['absolute_yardline_number'].between(20, 90)]

            clean_week_df = self.process_single_week(week_num, input_df, output_df, clean_context)

            if not clean_week_df.empty:
                processed_chunks.append(clean_week_df)

            # Explicit Memory Management
            del input_df, output_df
            gc.collect()

        if not processed_chunks:
            return pd.DataFrame()
        
        return pd.concat(processed_chunks, ignore_index=True)
import pandas as pd
import numpy as np
from scipy.signal import savgol_filter
from schema import PhysicsSchema

class PhysicsEngine:
    def __init__(self):
        self.output_schema = PhysicsSchema

    def derive_metrics(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Applies Savitzky-Golay filter to calculate s, a, and dir.
        Expects a clean, sorted, normalized DataFrame.
        """
        df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])
        
        # SAVITZKY-GOLAY PARAMETERS
        # Window = 7: Represents 0.7 seconds. Chosen to smooth sensor noise 
        #             while preserving the sharp "cuts" of NFL athletes.
        # Poly = 2:   Quadratic fit. Assumes constant acceleration within the 
        #             window, which prevents overfitting to high-frequency noise.
        WINDOW = 7
        POLY = 2
        
        def calculate_sg(group):
            # Short Track Handling
            if len(group) < WINDOW:
                return pd.DataFrame({
                    's_derived': np.nan, 'a_derived': np.nan, 'dir_derived': np.nan
                }, index=group.index)
            
            # 1. First Derivative (Velocity)
            vx = savgol_filter(group['x'], window_length=WINDOW, polyorder=POLY, deriv=1, delta=0.1)
            vy = savgol_filter(group['y'], window_length=WINDOW, polyorder=POLY, deriv=1, delta=0.1)
            
            # 2. Second Derivative (Acceleration)
            ax = savgol_filter(group['x'], window_length=WINDOW, polyorder=POLY, deriv=2, delta=0.1)
            ay = savgol_filter(group['y'], window_length=WINDOW, polyorder=POLY, deriv=2, delta=0.1)
            
            # 3. Magnitudes
            s = np.sqrt(vx**2 + vy**2)
            a = np.sqrt(ax**2 + ay**2) # Magnitude is always positive
            
            # 4. Direction (Angle)
            # Convert Math Angle (counter-clockwise from X) to NFL Angle (clockwise from Y)
            angle_rad = np.arctan2(vy, vx)
            angle_deg = np.degrees(angle_rad)
            dir_val = (90 - angle_deg) % 360
            
            # 5. Noise Masking
            # If speed is < 0.5 yds/s, direction is effectively random noise
            s_clean = pd.Series(s, index=group.index)
            dir_clean = pd.Series(dir_val, index=group.index)
            a_clean = pd.Series(a, index=group.index)
            
            dir_clean = dir_clean.mask(s_clean < 0.5, np.nan)
            
            return pd.DataFrame({
                's_derived': s_clean,
                'a_derived': a_clean,
                'dir_derived': dir_clean
            }, index=group.index)

        # Apply grouping
        mask_players = df['nfl_id'].notna()
        
        # This returns a DataFrame (physics_cols) indexed by the original df row index.
        physics_cols = df[mask_players].groupby(
            ['game_id', 'play_id', 'nfl_id'], group_keys=False).apply(calculate_sg)

        df.loc[physics_cols.index, 's_derived'] = physics_cols['s_derived']
        df.loc[physics_cols.index, 'a_derived'] = physics_cols['a_derived']
        df.loc[physics_cols.index, 'dir_derived'] = physics_cols['dir_derived']

        return self.output_schema.validate(df)
import pandas as pd
import numpy as np
from schema import VoidDetectiontSchema

class VoidDetector:
    def __init__(self):
        self.output_schema = VoidDetectiontSchema

    def generate_reaction_metrics(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculates the two core metrics (Reaction Time and Avg Error)
        and aggregates frame-level data into a single row per player/play.
        """
        # Filter: We only grade Defensive Coverage players in the Post-Throw phase
        mask = (df['player_role'] == 'Defensive Coverage') & (df['phase'] == 'post_throw')
        df_cov = df[mask].copy()
        
        # THE AGGREGATION ALGORITHM
        def analyze_reaction(group):
            # Ensure sorted by time (guaranteed by the previous pipeline steps)
            group = group.sort_values('frame_id')
            
            # Mask frames where the defender is 'Locked In'
            locked_in_mask = group['angle_diff'] < 45
            
            if not locked_in_mask.any():
                reaction_frames = len(group)
            else:
                # Find the frame ID of the first time they locked in
                first_lock_idx = locked_in_mask.idxmax()
                first_frame = group.loc[first_lock_idx, 'frame_id']
                start_frame = group['frame_id'].min() # Frame ID of the throw (usually 1)
                
                # Reaction time is measured in frame counts
                reaction_frames = first_frame - start_frame
            
            avg_error = group['angle_diff'].mean()
            
            return pd.Series({
                'reaction_time_frames': reaction_frames,
                'avg_pursuit_error': avg_error
            })

        results_agg = df_cov.groupby(
            ['game_id', 'play_id', 'nfl_id'], group_keys=False).apply(analyze_reaction)
        
        # CLEANUP & VOID FLAGGING
        results = results_agg.reset_index() 

        # 6 frames = 0.6 seconds. If reaction time is greater than 0.6s, it's a Void.
        results['is_reaction_void'] = results['reaction_time_frames'] > 4
        results['nfl_id'] = results['nfl_id'].astype(float)

        # Validate the output before passing it back
        return self.output_schema.validate(results)
import os
from datetime import datetime
import pandas as pd
import gc

from load_data import DataLoader
from data_preprocessor import DataPreProcessor
from physics_engine import PhysicsEngine
from feature_engineering import VectorFeatureEngine
from void_detector import VoidDetector

def run_full_pipeline():
    start_time = datetime.now()

    DATA_DIR = 'data/train'
    SUPP_FILE = 'data/supplementary_data.csv'
    OUTPUT_DIR = 'data/processed'

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 1. Loading & Streaming
    print("[1/6] Loading Data Streams...")
    loader = DataLoader(DATA_DIR, SUPP_FILE)
    raw_supp = loader.load_supplementary()
    raw_tracking = loader.stream_weeks()
    
    # 2. PREPROCESSING (Clean, Stitch, Normalize)
    print("[2/6] Preprocessing & Stitching frames...")
    processor = DataPreProcessor()
    df_clean = processor.run(
        data_stream=raw_tracking, 
        raw_context_df=raw_supp
    )

    # print(df_clean.shape)
    # return
    # 3. PHYSICS ENGINE (Derive S, A, Dir)
    print("[3/6] Running Physics Engine (Savitzky-Golay)...")
    physics_engine = PhysicsEngine()
    df_physics = physics_engine.derive_metrics(df_clean)
    
    # Explicit memory cleanup
    del df_clean
    gc.collect() 

    # 4. VECTOR ENGINEERING (Calculate Ideal Vector)
    print("[4/6] Calculating Pursuit Vectors...")
    vector_engine = VectorFeatureEngine()
    df_features = vector_engine.calculate_pursuit_vectors(df_physics)
    
    # 5. METRICS (Aggregate to Player/Play Level)
    print("[5/6] Detecting Reaction Voids...")
    void_detector = VoidDetector()
    df_metrics = void_detector.generate_reaction_metrics(df_features)

    print("[6/6] Reconciling Context and Exporting...")

    # We grab essential play/player context from the massive frame file (df_features)
    CONTEXT_COLUMNS = [
        'game_id', 'play_id', 'nfl_id', 'player_name', 'player_position', 
        'week', 'down', 'yards_to_go', 'pass_result', 'expected_points_added'
    ]
    # We drop duplicates to get one row per player/play, which is safe since context is constant
    context_lookup = df_features[CONTEXT_COLUMNS].drop_duplicates(subset=['game_id', 'play_id', 'nfl_id'])
    
    # B. Merge 1: Create the FINAL Analytical Summary Report
    # Merge metrics with full context (week, player name, EPA, etc.)
    df_summary = df_metrics.merge(
        context_lookup, 
        on=['game_id', 'play_id', 'nfl_id'], 
        how='left'
    )
    
    # Final EPA/Punishment Calculation (Now that we have expected_points_added)
    df_summary['damage_epa'] = df_summary['expected_points_added'].clip(lower=0).fillna(0.0)
    df_summary['is_punished'] = df_summary['damage_epa'] > 0
    df_summary = df_summary.drop(columns=['expected_points_added'])
    
    summary_path = os.path.join(OUTPUT_DIR, 'reaction_analysis_summary.csv')
    df_summary.to_csv(summary_path, index=False)
    print(f"   -> Saved Analytical Report to {summary_path}")

    # C. Merge 2: Create the Animation Master File
    # We merge the final flags (is_reaction_void, etc.) back onto the frame data (df_features)
    # The dataframe will contain all frame-level data (x, y, s_derived, angle_diff) + the final flag
    animation_cols = ['game_id', 'play_id', 'nfl_id', 'is_reaction_void', 'is_punished', 'damage_epa']
    flags_to_merge = df_summary[animation_cols]
    
    df_animation = df_features.merge(
        flags_to_merge, 
        on=['game_id', 'play_id', 'nfl_id'], 
        how='left'
    )
    
    # Final cleanup for animation NAs (offense/ball)
    df_animation['is_reaction_void'] = df_animation['is_reaction_void'].fillna(False)
    df_animation['is_reaction_void'] = df_animation['is_reaction_void'].astype(bool)

    final_path = os.path.join(OUTPUT_DIR, 'master_animation_data.csv')
    df_animation.to_csv(final_path, index=False)
    print(f"   -> Saved Animation Master File to {final_path}")
    
    duration = datetime.now() - start_time
    print(f"üèÅ PIPELINE FINISHED in {duration}")

if __name__ == "__main__":
    run_full_pipeline()
